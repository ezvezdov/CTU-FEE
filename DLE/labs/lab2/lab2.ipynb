{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from toy_model import *\n",
    "# np.random.seed(seed=1)\n",
    "# torch.manual_seed(1)\n",
    "G = G2Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Tensor basics (2p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative of l w.r.t. y is  (tensor(6.),)\n",
      "w.grad value is  tensor(38.)\n",
      "Initial w:  tensor(1.)\n",
      "Changed w:  tensor(-2.8000)\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(1, dtype = torch.float32)\n",
    "x = torch.tensor(2.0)\n",
    "t = torch.tensor(np.float32(3))\n",
    "b = torch.tensor(4, dtype = torch.float32)\n",
    "\n",
    "w.requires_grad = True\n",
    "a = x + b\n",
    "y = torch.maximum(a*w,torch.tensor(0))\n",
    "l = torch.pow(y - t, 2) + torch.pow(w,2)\n",
    "\n",
    "print(\"Derivative of l w.r.t. y is \", torch.autograd.grad(l,y,retain_graph=True))\n",
    "\n",
    "l.backward() # Compute derivative w.r.t all leaf variables\n",
    "print(\"w.grad value is \",w.grad)\n",
    "\n",
    "print(\"Initial w: \", w.data)\n",
    "with torch.no_grad():\n",
    "    w = w - 0.1*w.grad\n",
    "print(\"Changed w: \", w.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_forward(x,y,w1,b1,w2,b2):\n",
    "    # Calculating linear layers\n",
    "    linear1 = torch.tanh(torch.matmul(x, w1.t()) + b1)\n",
    "    linear2 = (torch.matmul(w2, linear1.t()) + b2).T\n",
    "\n",
    "    # Calculating loss\n",
    "    loss = torch.mean(F.logsigmoid(linear2 * y))\n",
    "\n",
    "    return loss\n",
    "\n",
    "N = 40\n",
    "\n",
    "# Generate train/test data\n",
    "train_data = G.generate_sample(N)\n",
    "x_train,y_train = train_data\n",
    "\n",
    "# Setting train data\n",
    "x = torch.from_numpy(x_train)\n",
    "x.float()\n",
    "y = torch.from_numpy(y_train)\n",
    "y = y.T\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected m1 and m2 to have the same dtype, but got: double != float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[511], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m w1,b1,w2,b2\n\u001b[1;32m     15\u001b[0m w1,b1,w2,b2 \u001b[38;5;241m=\u001b[39m generate_params(x, hidden_size,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mnn_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[510], line 3\u001b[0m, in \u001b[0;36mnn_forward\u001b[0;34m(x, y, w1, b1, w2, b2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnn_forward\u001b[39m(x,y,w1,b1,w2,b2):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Calculating linear layers\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     linear1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1)\n\u001b[1;32m      4\u001b[0m     linear2 \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mmatmul(w2, linear1\u001b[38;5;241m.\u001b[39mt()) \u001b[38;5;241m+\u001b[39m b2)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Calculating loss\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected m1 and m2 to have the same dtype, but got: double != float"
     ]
    }
   ],
   "source": [
    "def generate_params(x,hidden_size, dtype):\n",
    "\n",
    "    # Setting parameters\n",
    "    w1 = torch.rand(hidden_size, x.shape[1],dtype=dtype) * 2 - 1\n",
    "    b1 = torch.rand(hidden_size,dtype=dtype) * 2 - 1\n",
    "    w2 = torch.rand(1, hidden_size,dtype=dtype) * 2 - 1\n",
    "    b2 = torch.rand(1,dtype=dtype) * 2 - 1\n",
    "\n",
    "    w1.requires_grad=True\n",
    "    b1.requires_grad=True\n",
    "    w2.requires_grad=True\n",
    "    b2.requires_grad=True\n",
    "    return w1,b1,w2,b2\n",
    "\n",
    "w1,b1,w2,b2 = generate_params(x, hidden_size,dtype=torch.float32)\n",
    "loss = nn_forward(x,y,w1,b1,w2,b2)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon : 0.0001, dtype: torch.float32\n",
      "Grad error in w1: 0.06556510925292969\n",
      "Grad error in b1: 0.013113021850585938\n",
      "Grad error in w2: -0.035762786865234375\n",
      "Grad error in b2: -0.30040740966796875\n",
      "torch.Size([500, 2])\n",
      "torch.Size([500, 2])\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1e-4\n",
    "\n",
    "def normalize_tensor(tensor):\n",
    "  \"\"\"Normalizes a tensor to have a norm of 1.\"\"\"\n",
    "  return tensor / tensor.norm()\n",
    "\n",
    "def calc_grad(loss1,loss2,epsilon):\n",
    "  return (loss1 - loss2) / (2*epsilon)\n",
    "\n",
    "\n",
    "u_w1 = normalize_tensor(torch.rand(w1.shape,dtype=torch.float32) * 2 - 1)\n",
    "u_b1 = normalize_tensor(torch.rand(b1.shape,dtype=torch.float32) * 2 - 1)\n",
    "u_w2 = normalize_tensor(torch.rand(w2.shape,dtype=torch.float32) * 2 - 1)\n",
    "u_b2 = normalize_tensor(torch.rand(b2.shape,dtype=torch.float32) * 2 - 1)\n",
    "\n",
    "g_w1 = calc_grad(nn_forward(x,y,w1 + epsilon * u_w1,b1,w2,b2), nn_forward(x,y,w1 - epsilon * u_w1,b1,w2,b2), epsilon)\n",
    "g_b1 = calc_grad(nn_forward(x,y,w1,b1 + epsilon * u_b1,w2,b2), nn_forward(x,y,w1,b1 - epsilon * u_b1,w2,b2), epsilon)\n",
    "g_w2 = calc_grad(nn_forward(x,y,w1,b1,w2 + epsilon * u_w2,b2), nn_forward(x,y,w1,b1,w2 - epsilon * u_w2,b2), epsilon)\n",
    "g_b2 = calc_grad(nn_forward(x,y,w1,b1,w2,b2 + epsilon * u_b2), nn_forward(x,y,w1,b1,w2,b2 - epsilon * u_b2), epsilon)\n",
    "\n",
    "print(f\"epsilon : {epsilon}, dtype: torch.float32\")\n",
    "print(f\"Grad error in w1: {g_w1}\")\n",
    "print(f\"Grad error in b1: {g_b1}\")\n",
    "print(f\"Grad error in w2: {g_w2}\")\n",
    "print(f\"Grad error in b2: {g_b2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(x_train)\n",
    "x = x.double()\n",
    "\n",
    "# Setting parameters\n",
    "w1 = torch.rand(hidden_size, x.shape[1],dtype=torch.float64) * 2 - 1\n",
    "b1 = torch.rand(hidden_size,dtype=torch.float64) * 2 - 1\n",
    "w2 = torch.rand(1, hidden_size,dtype=torch.float64) * 2 - 1\n",
    "b2 = torch.rand(1,dtype=torch.float64) * 2 - 1\n",
    "\n",
    "w1.requires_grad=True\n",
    "b1.requires_grad=True\n",
    "w2.requires_grad=True\n",
    "b2.requires_grad=True\n",
    "\n",
    "loss = nn_forward(x,y,w1,b1,w2,b2)\n",
    "\n",
    "# Backward\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon : 1e-05, dtype: torch.float64\n",
      "Grad error in w1: 0.1279879019433494\n",
      "Grad error in b1: 0.03840802764720763\n",
      "Grad error in w2: 0.18297967032676607\n",
      "Grad error in b2: 0.018187132155844665\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1e-5\n",
    "\n",
    "def normalize_tensor(tensor):\n",
    "  \"\"\"Normalizes a tensor to have a norm of 1.\"\"\"\n",
    "  return tensor / tensor.norm()\n",
    "\n",
    "def calc_grad(loss1,loss2,epsilon):\n",
    "  return (loss1 - loss2) / (2*epsilon)\n",
    "\n",
    "\n",
    "u_w1 = normalize_tensor(torch.rand(w1.shape,dtype=torch.float64) * 2 - 1)\n",
    "u_b1 = normalize_tensor(torch.rand(b1.shape,dtype=torch.float64) * 2 - 1)\n",
    "u_w2 = normalize_tensor(torch.rand(w2.shape,dtype=torch.float64) * 2 - 1)\n",
    "u_b2 = normalize_tensor(torch.rand(b2.shape,dtype=torch.float64) * 2 - 1)\n",
    "\n",
    "x.double()\n",
    "\n",
    "g_w1 = calc_grad(nn_forward(x,y,w1 + epsilon * u_w1,b1,w2,b2), nn_forward(x,y,w1 - epsilon * u_w1,b1,w2,b2), epsilon)\n",
    "g_b1 = calc_grad(nn_forward(x,y,w1,b1 + epsilon * u_b1,w2,b2), nn_forward(x,y,w1,b1 - epsilon * u_b1,w2,b2), epsilon)\n",
    "g_w2 = calc_grad(nn_forward(x,y,w1,b1,w2 + epsilon * u_w2,b2), nn_forward(x,y,w1,b1,w2 - epsilon * u_w2,b2), epsilon)\n",
    "g_b2 = calc_grad(nn_forward(x,y,w1,b1,w2,b2 + epsilon * u_b2), nn_forward(x,y,w1,b1,w2,b2 - epsilon * u_b2), epsilon)\n",
    "\n",
    "print(f\"epsilon : {epsilon}, dtype: torch.float64\")\n",
    "print(f\"Grad error in w1: {g_w1}\")\n",
    "print(f\"Grad error in b1: {g_b1}\")\n",
    "print(f\"Grad error in w2: {g_w2}\")\n",
    "print(f\"Grad error in b2: {g_b2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "\n",
    "# Generate train/test data\n",
    "train_data = G.generate_sample(N)\n",
    "x_train,y_train = train_data\n",
    "\n",
    "# Setting train data\n",
    "x = torch.from_numpy(x_train)\n",
    "y = torch.from_numpy(y_train)\n",
    "y = y.T\n",
    "\n",
    "def train(hidden_size,lr=0.1,epoch=1000):\n",
    "    w1,b1,w2,b2 = generate_params(x,hidden_size,dtype=torch.float32)\n",
    "    for ep in epoch:\n",
    "        loss = nn_forward(x,y,w1,b1,w2,b2)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            w1 = w1 - lr * w1.grad\n",
    "            b1 = b1 - lr * b1.grad\n",
    "            w2 = w2 - lr * w2.grad\n",
    "            b2 = b2 - lr * b2.grad\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "w1, b1, w2, b2 = train(hidden_size=5)\n",
    "\n",
    "# I don't know how to use it with my data, because I didn't know that I should use \"main_template.py\"\n",
    "G2Model.plot_predictor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
