{"cells":[{"cell_type":"markdown","metadata":{"id":"HV_h0NBNrh6G"},"source":["# Lab 3: Partially Observable Markov Decision Process (POMDP)\n","\n","In the previous lab, we studied the MDP model:\n","\n","- $\\mathcal S$ - state space\n","- $\\mathcal A$ - possible actions\n","- $T$ - transition function\n","- $R$ - reward function\n","\n","POMDP extends MDP with:\n","\n","- set of possible observations,\n","- observation function or distribution O(o|s',a),\n","- and initial belief distribution over the states.\n","\n","The goal is to maximize reward in a partially observable environment.\n","\n","- Agent cannot fully observe current state but instead has belief in form of probability distribution over the entire state space.\n","- For each action and new state agent receives an observation.\n","- Agent updates its belief based on the previous belief, action and observation of the new state.\n","\n","\n","Example:\n","\n","- Maze: Agent only see few tiles around him. (Maze is foggy)\n","- Can you think of other examples?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qcI3VTrjrh6I"},"source":["## Task 1\n","\n","Let's use the maze again but differently:\n","\n","- Direction of agent is now part of the state. Agent in the maze is represented by characters `<` `>` `^` `v`  based on its direction, instead of `@`\n","- Agent actions (turn left `l`, turn right `r`) can change its direction.\n","- Only movement forward `m` is allowed.\n","\n","Implement function `apply` in this new setup."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"sWaaPOPurh6I"},"outputs":[],"source":["from copy import deepcopy\n","from typing import List, Tuple, Dict, Set\n","\n","\n","Action = str  # Type alias\n","\n","\n","class State:\n","\n","    def __init__(self, maze_rows: List[str]):\n","        self._maze_rows = [list(row) for row in maze_rows]\n","\n","        # Make sure the maze has a rectangular shape, and has a proper boundary.\n","        assert all(len(row) == self.num_cols() for row in self._maze_rows)\n","        assert set(self._maze_rows[0]) == {\"#\"} and set(self._maze_rows[-1]) == {\"#\"}\n","        assert all(row[0] == \"#\" and row[-1] == \"#\" for row in self._maze_rows)\n","\n","    def actions(self) -> List[Action]:\n","        \"\"\"\n","        :return: list of actions available at the current state.\n","        \"\"\"\n","        return [\n","            \"m\",  # Move forward.\n","            \"l\",  # Turn left.\n","            \"r\",  # Turn right.\n","        ]\n","\n","    def apply(self, action: Action) -> None:\n","        \"\"\"\n","        Apply action in the current state.\n","        \"\"\"\n","        i, j, direction = self.current_position()\n","\n","        empty_place = \" \"\n","        d = [\"^\",\">\",\"v\",\"<\"]\n","\n","        if action == \"m\":\n","            \n","            if direction == \"^\" and self._maze_rows[i-1][j] != \"#\":\n","                self._maze_rows[i][j] = empty_place\n","                self._maze_rows[i-1][j] = direction\n","            elif direction == \">\" and self._maze_rows[i][j+1] != \"#\":\n","                self._maze_rows[i][j] = empty_place\n","                self._maze_rows[i][j+1] = direction\n","            elif direction == \"v\" and self._maze_rows[i+1][j] != \"#\":\n","                self._maze_rows[i][j] = empty_place\n","                self._maze_rows[i+1][j] = direction\n","            elif direction == \"<\" and self._maze_rows[i][j-1] != \"#\":\n","                self._maze_rows[i][j] = empty_place\n","                self._maze_rows[i][j-1] = direction\n","        elif action == \"l\":\n","            self._maze_rows[i][j] = d[(d.index(direction)-1) % len(d)]\n","        elif action == \"r\":\n","            self._maze_rows[i][j] = d[(d.index(direction)+1) % len(d)]\n","\n","        return\n","\n","    def copy(self) -> \"State\":\n","        return State(deepcopy(self._maze_rows))\n","\n","    # -- Maze specific methods ---------------------------------------------------\n","\n","    def num_rows(self) -> int:\n","        return len(self._maze_rows)\n","\n","    def num_cols(self) -> int:\n","        return len(self._maze_rows[0])\n","\n","    def current_position(self) -> Tuple[int, int, str]:\n","        # Return agent coordinates and orientation.\n","        for i in range(self.num_rows()):\n","            for j in range(self.num_cols()):\n","                if self._maze_rows[i][j] in [\"^\", \"<\", \"v\", \">\"]:\n","                    return i, j, self._maze_rows[i][j]\n","        raise RuntimeError(\n","            \"Invalid maze: current position not found in: \" + \"\\n\".join(self._maze_rows)\n","        )\n","    \n","    def num_golds(self) -> int:\n","        golds = 0\n","        for i in range(self.num_rows()):\n","            for j in range(self.num_cols()):\n","                if self._maze_rows[i][j] == \"G\":\n","                    golds += 1\n","        return golds\n","    \n","    def has_any_gold(self) -> bool:\n","        return self.num_golds() > 0\n","\n","    # -- Helper methods ----------------------------------------------------------\n","\n","    def __str__(self) -> str:\n","        return \"\\n\".join([\"\".join(row) for row in self._maze_rows])\n","\n","    def __eq__(self, other) -> bool:\n","        return self._maze_rows == other._maze_rows\n","\n","    def __hash__(self) -> int:\n","        return hash(str(self))\n","\n","    def __repr__(self) -> str:\n","        return str(self)\n"]},{"cell_type":"markdown","metadata":{"id":"GIlKbt4vBF72"},"source":["We provide a number of test cases your implementation must pass:"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"fL7eM2Bcrh6J"},"outputs":[],"source":["def test_apply_turn() -> None:\n","    test_maze = [\n","        \"#####\",\n","        \"# ^ #\",\n","        \"#####\"]\n","\n","    maze = State(test_maze)\n","    maze.apply(\"l\")\n","    result_maze = [\n","        \"#####\",\n","        \"# < #\",\n","        \"#####\" ]\n","    assert maze == State(result_maze)\n","\n","    maze = State(test_maze)\n","    maze.apply(\"r\")\n","    result_maze = [\n","        \"#####\",\n","        \"# > #\",\n","        \"#####\" ]\n","    assert maze == State(result_maze)\n","\n","    maze = State(test_maze)\n","    maze.apply(\"l\")\n","    maze.apply(\"l\")\n","    result_maze = [\n","        \"#####\",\n","        \"# v #\",\n","        \"#####\" ]\n","    assert maze == State(result_maze)\n","\n","    maze = State(test_maze)\n","    maze.apply(\"l\")\n","    maze.apply(\"r\")\n","    result_maze = [\n","        \"#####\",\n","        \"# ^ #\",\n","        \"#####\" ]\n","    assert maze == State(result_maze)\n","    \n","def test_apply_move() -> None:\n","    test_maze = [\n","        \"#####\",\n","        \"# ^ #\",\n","        \"#####\"]\n","\n","    maze = State(test_maze)\n","    maze.apply(\"m\")\n","    result_maze = [\n","        \"#####\",\n","        \"# ^ #\",\n","        \"#####\" ]\n","    assert maze == State(result_maze)\n","\n","    test_maze = [\n","        \"#####\",\n","        \"# > #\",\n","        \"#####\"]\n","    maze = State(test_maze)\n","    maze.apply(\"m\")\n","    result_maze = [\n","        \"#####\",\n","        \"#  >#\",\n","        \"#####\" ]\n","    assert maze == State(result_maze)\n","    \n","    \n","def test_apply_move_gold() -> None:\n","    test_maze = [\n","        \"#####\",\n","        \"# >G#\",\n","        \"#####\"]\n","    maze = State(test_maze)\n","    assert maze.num_golds() == 1\n","    maze.apply(\"m\")\n","    result_maze = [\n","        \"#####\",\n","        \"#  >#\",\n","        \"#####\" ]\n","    assert maze == State(result_maze)\n","    assert maze.num_golds() == 0\n","\n","# Pass tests\n","test_apply_turn()\n","test_apply_move()\n","test_apply_move_gold()"]},{"cell_type":"markdown","metadata":{"id":"ZsxjMJG-rh6M"},"source":["# Models\n","\n","\n","In Lab2, we had a transition model in the form of a `SxSxA` matrix and a reward model in the form of a `SxA` matrix. Keeping all transitions and rewards in the memory can cause memory issues for large problems. In the case of POMDPs, we need to have yet another matrix for all the observations (`SxA` matrix in general case).\n","\n","A different approach we will now use is to procedurally generate transitions, rewards and observation only when needed, based on known rules. In this way, we don't need to keep the whole matrix in memory all the time."]},{"cell_type":"markdown","metadata":{"id":"POdo5vocrh6N"},"source":["### Solver library\n","- To solve instances of POMDP, we will use `pomdp_py` library.\n","\n","- The library uses `pomdp_py.State`, `pomdp_py.Observation`, `pomdp_py.Action` classes to represent the corresponding concepts. These create an interface we should work with in order to use the library -- therefore we will wrap our implementation inside of these classes.\n","\n","- Additionally, we need to define `TransitionModel`, `RewardModel` and `ObservationModel` classes to model corresponding distributions. \n","\n","- We need function `get_all_states` to create uniform prior belief over all states."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"n6GsCapXrh6M"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: cython in /usr/lib/python3.10/site-packages (0.29.33)\n","Defaulting to user installation because normal site-packages is not writeable\n","Collecting git+https://github.com/h2r/pomdp-py.git\n","  Cloning https://github.com/h2r/pomdp-py.git to /tmp/pip-req-build-ab_0_83a\n","  Running command git clone --filter=blob:none --quiet https://github.com/h2r/pomdp-py.git /tmp/pip-req-build-ab_0_83a\n","  Resolved https://github.com/h2r/pomdp-py.git to commit d1b733a0fe21b4fe41522b5ade4c3656a417164d\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: Cython in /usr/lib/python3.10/site-packages (from pomdp-py==1.3.2) (0.29.33)\n","Requirement already satisfied: numpy in /usr/lib/python3.10/site-packages (from pomdp-py==1.3.2) (1.24.2)\n","Requirement already satisfied: scipy in /home/ezvezdov/.local/lib/python3.10/site-packages (from pomdp-py==1.3.2) (1.10.1)\n","Requirement already satisfied: tqdm in /usr/lib/python3.10/site-packages (from pomdp-py==1.3.2) (4.65.0)\n","Requirement already satisfied: matplotlib in /usr/lib/python3.10/site-packages (from pomdp-py==1.3.2) (3.6.3)\n","Collecting pygame\n","  Downloading pygame-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting opencv-python\n","  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n","\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/lib/python3.10/site-packages (from matplotlib->pomdp-py==1.3.2) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/lib/python3.10/site-packages (from matplotlib->pomdp-py==1.3.2) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/lib/python3.10/site-packages (from matplotlib->pomdp-py==1.3.2) (4.39.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/lib/python3.10/site-packages (from matplotlib->pomdp-py==1.3.2) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/lib/python3.10/site-packages (from matplotlib->pomdp-py==1.3.2) (23.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3.10/site-packages (from matplotlib->pomdp-py==1.3.2) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3.10/site-packages (from matplotlib->pomdp-py==1.3.2) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3.10/site-packages (from matplotlib->pomdp-py==1.3.2) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->pomdp-py==1.3.2) (1.16.0)\n","Building wheels for collected packages: pomdp-py\n","  Building wheel for pomdp-py (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for pomdp-py: filename=pomdp_py-1.3.2-cp310-cp310-linux_x86_64.whl size=4174976 sha256=91c11fd0adcd546594243eabc9168e2cf9bdea5368c1d4e6b071dedc0ef1d655\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-fr0081t1/wheels/6f/8d/c6/9df1c024259f303b3336d1943594547232b3da75879fde5dee\n","Successfully built pomdp-py\n","Installing collected packages: pygame, opencv-python, pomdp-py\n","Successfully installed opencv-python-4.7.0.72 pomdp-py-1.3.2 pygame-2.2.0\n"]}],"source":["# Install library with pomdp solver (takes a while) ...\n","!pip install cython\n","!pip install git+https://github.com/h2r/pomdp-py.git"]},{"cell_type":"markdown","metadata":{"id":"P_wb604uBF74"},"source":["We need to implement interface for the pomdp library:"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"CnS6nfTbBF74"},"outputs":[],"source":["import pomdp_py"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"-t6zbWoGBF74"},"outputs":[],"source":["# This is boilerplate code that wraps our State and Action\n","# for the use with the external library.\n","\n","class PomdpAction(pomdp_py.Action):\n","    def __init__(self, value: Action) -> None: \n","      self.value = value\n","    \n","    def __hash__(self) -> int:         \n","      return hash(self.value)\n","    \n","    def __eq__(self, other) -> bool:    \n","      return self.value == other.value\n","    \n","    def __str__(self) -> str:          \n","      return self.value\n","    \n","    def __repr__(self) -> str:         \n","      return str(self)\n","      \n","\n","class PomdpState(pomdp_py.State):\n","    def __init__(self, maze_rows: List[str]) -> None:\n","        super().__init__()\n","        self.state = State(maze_rows)\n","        self._maze_rows = self.state._maze_rows\n","    \n","    def apply(self, action: PomdpAction) -> None: \n","      self.state.apply(action.value)\n","    \n","    def actions(self) -> List[PomdpAction]:              \n","      return [PomdpAction(\"m\"), PomdpAction(\"l\"), PomdpAction(\"r\")]\n","    \n","    def current_position(self) -> Tuple[int, int, str]: \n","      return self.state.current_position()\n","    \n","    def num_golds(self) -> int:     \n","      return self.state.num_golds()\n","    \n","    def has_any_gold(self) -> bool: \n","      return self.state.has_any_gold()\n","    \n","    def copy(self) -> \"PomdpState\": \n","      return PomdpState(deepcopy(self.state._maze_rows))\n","    \n","    def __hash__(self) -> int:             \n","      return hash(self.state)\n","    \n","    def __eq__(self, other: \"PomdpState\") -> bool:\n","      return self.state == other.state\n","    \n","    def __str__(self) -> str:\n","      return str(self.state)\n","    \n","    def __repr__(self) -> str:\n","      return repr(self.state)\n","\n","\n","class PomdpObservation(pomdp_py.Observation):\n","    def __init__(self, value) -> None:  \n","      self.value = value\n","\n","    def __hash__(self) -> int:         \n","      return hash(self.value)\n","    \n","    def __eq__(self, other: \"PomdpState\") :    \n","      return self.value == other.value\n","      \n","    def __str__(self) -> str:          \n","      return self.value\n","\n","    def __repr__(self) -> str:         \n","      return str(self)\n","\n","    \n","class PolicyModel(pomdp_py.RandomRollout):\n","    def sample(self, state, **kwargs) -> Action:\n","        return self.get_all_actions().random()\n","\n","    def get_all_actions(self, **kwargs) -> List[Action]:\n","        return [PomdpAction(\"m\"), PomdpAction(\"l\"), PomdpAction(\"r\")]"]},{"cell_type":"markdown","metadata":{"id":"Ot-qeZ3DBF74"},"source":["**From now on, we will use these wrapped classes**: `PomdpAction`, `PomdpState`, `PomdpObservation`, `PolicyModel`"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"2WTA_4HMBF74"},"outputs":[],"source":["# Slightly modified function get_all_transitions() from Lab 2.\n","\n","def get_all_states(init_state: PomdpState) -> List[PomdpState]:\n","    states = [init_state]\n","    opened = [0]\n","\n","    while len(opened) > 0:\n","        from_idx = opened.pop()\n","        state = states[from_idx]\n","\n","        for action in state.actions():\n","            next_state = state.copy()\n","            next_state.apply(action)\n","            if next_state in states:\n","                # State was visited.\n","                to_idx = states.index(next_state)\n","            else:\n","                # Not visited yet, add to open.\n","                states.append(next_state)\n","                to_idx = len(states) - 1\n","                opened.append(to_idx)\n","    return states"]},{"cell_type":"markdown","metadata":{"id":"vUQCuttUrh6O"},"source":["\n","# Task 2\n","\n","Implement probability(**next_state**, **state**, **action**) method of transition model.\n","\n","In our deterministic case probability is always 1 (if **action** from **state** leads to **next_state**) or 0 (if not)."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"2eGDJ5m6rh6P"},"outputs":[],"source":["class TransitionModel(pomdp_py.TransitionModel):\n","    \"\"\"\n","    Models the distribution T(s, a, s') = Pr(s'|s,a).\n","    In our deterministic case Pr(s'|s,a) is always 1 \n","    (if s,a goes to s') or 0 (if not).\n","    \"\"\"\n","\n","    def probability(\n","        self, \n","        next_state: PomdpState, \n","        state: PomdpState, \n","        action: PomdpAction\n","    ) -> float:\n","        \"\"\"\n","        Returns the probability of Pr(s'|s,a)\n","        \"\"\"\n","        maze_copy = deepcopy(state)\n","        maze_copy.apply(action)\n","        return int(maze_copy == next_state)\n","\n","    def sample(\n","          self, \n","          state: PomdpState, \n","          action: PomdpAction\n","    ) -> PomdpState:\n","        \"\"\"\n","        Given state and action, return next state.\n","        \"\"\"\n","        next_state = state.copy()\n","        next_state.apply(action)\n","        return next_state\n","    \n","    def get_all_states(self) -> List[PomdpState]:\n","        return self.states"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"MY0QJk--rh6P"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","1\n","######\n","# < G#\n","######\n"]}],"source":["# Example. Try to predict the output values before running!\n","\n","tm = TransitionModel()\n","\n","state1 = PomdpState([\n","    \"######\",\n","    \"# ^ G#\",\n","    \"######\"])\n","\n","state2 = PomdpState([\n","    \"######\",\n","    \"# > G#\",\n","    \"######\"])\n","\n","print(tm.probability(state2, state1, PomdpAction(\"l\")))\n","print(tm.probability(state2, state1, PomdpAction(\"r\")))\n","print(tm.sample(state1, PomdpAction(\"l\")))\n"]},{"cell_type":"markdown","metadata":{"id":"LpXTapWjrh6P"},"source":["# Task 3\n","\n","Implement sample(***state***, ***action***) method of reward model.\n","\n","If ***action*** leads to ***state*** with gold, reward is 1. In all other cases, reward is -0.1\n","Hint: use gold attribute from Maze."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"U25IqIkurh6P"},"outputs":[],"source":["class RewardModel(pomdp_py.RewardModel):\n","    \"\"\"\n","    Models rewards.\n","    In our case a reaward is 1 (if a leads to gold state) \n","    or -0.1 (if not).\n","    \"\"\"\n","\n","    def sample(\n","        self, \n","        state: PomdpState, \n","        action: PomdpAction, \n","        next_state=None\n","    ) -> float:\n","        \"\"\"\n","        Given state and action, return reward. \n","        Next state is ignored in our Maze env.\n","        \"\"\"\n","        next_state = state.copy()\n","        next_state.apply(action)\n","        if next_state.num_golds() < state.num_golds():\n","          return 1.0\n","        else:\n","          return -0.1\n","        \n","        \n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"8eiAWq6Trh6Q"},"outputs":[{"name":"stdout","output_type":"stream","text":["#####\n","# ^G#\n","#####\n","Action r Reward -0.1\n","#####\n","# >G#\n","#####\n","\n","\n","#####\n","# >G#\n","#####\n","Action m Reward 1.0\n","#####\n","#  >#\n","#####\n"]}],"source":["# Example:\n","\n","rm = RewardModel()\n","maze = PomdpState([\n","    \"#####\",\n","    \"# ^G#\",\n","    \"#####\"])\n","\n","action = PomdpAction(\"r\")\n","print(maze)\n","print(f'Action {action} Reward {rm.sample(maze, action, None)}')\n","maze.apply(action)\n","print(maze)\n","print('\\n')\n","\n","action = PomdpAction(\"m\")\n","print(maze)\n","print(f'Action {action} Reward {rm.sample(maze, action, None)}')\n","maze.apply(action)\n","print(maze)\n"]},{"cell_type":"markdown","metadata":{"id":"gbl16eu6rh6R"},"source":["# Task 4\n","\n","\n","Implement function observe(**state**).\n","\n","- Function should output string of 4 character based on building blocks of maze, i.e.  one of `'#', ' ', 'G'`\n","- Example output: `\" # #\"`\n","- Function output depends on the state and uses the direction of the agent. (Select characters clockwise, starting with the character in front of the agent.)\n"]},{"cell_type":"markdown","metadata":{"id":"pfGu3ktTBF75"},"source":["Examples. What are the observations of the agent?\n","\n","```\n","##1##\n","#4^2#\n","##3##\n","```\n","\n","\n","\n","```\n","#####\n","# ^G#\n","#####\n","```\n","\n","\n","```\n","#####\n","# >G#\n","#####\n","```\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"UFDPPgK_rh6R"},"outputs":[],"source":["class ObservationModel(pomdp_py.ObservationModel):\n","    '''\n","    Models distribution O(s',a,o) = Pr(o|s',a)\n","    In our case, observation does not depend on action -  Pr(o|s') \n","    (All actions leads to some observation).\n","    \n","    Our Observation model is deterministic: Pr(o|s') is always 1 \n","    (if observation of s' matches o) or 0 (if not).\n","    '''\n","\n","    def probability(\n","        self, \n","        observation: PomdpObservation, \n","        next_state: PomdpState, \n","        action: PomdpAction=None\n","    ) -> float:\n","        '''\n","        Returns the probability Pr(o|s',a).\n","        '''\n","        next_state_observation = self.sample(next_state)\n","        if next_state_observation == observation:\n","            return 1.0\n","        else:\n","            return 0.0\n","\n","    def observe(self, state: PomdpState) -> str:\n","        '''\n","        Agent looks around clockwise and returns string of 4 characters.\n","        '''\n","        i,j,direction = state.current_position()\n","        maze = state._maze_rows\n","\n","        d = [\"^\",\">\",\"v\",\"<\"] # possible directions\n","        around = [ maze[i-1][j], maze[i][j+1], maze[i+1][j], maze[i][j-1]]    \n","        str =\"\"\n","\n","        idx = d.index(direction)\n","        \n","        while len(str) != len(d):\n","            str += around[idx]\n","            idx = (idx+1) % len(d)\n","        return str\n","        \n","    def sample(\n","        self, \n","        next_state: PomdpState, \n","        action: PomdpAction=None\n","        ) -> PomdpObservation:\n","        return PomdpObservation(self.observe(next_state))\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"l45k9SZjrh6R"},"outputs":[],"source":["def test_observation():\n","    om = ObservationModel()\n","    maze = PomdpState([\n","        \"#####\",\n","        \"# ^G#\",\n","        \"#####\"])\n","    assert str(om.sample(maze)) == '#G# '\n","\n","    maze = PomdpState([\n","        \"#####\",\n","        \"# >G#\",\n","        \"#####\"])\n","    assert str(om.sample(maze)) == 'G# #'\n","\n","    maze = PomdpState([\n","        \"#####\",\n","        \"# vG#\",\n","        \"#####\"])\n","    assert str(om.sample(maze)) == '# #G'\n","\n","    maze = PomdpState([\n","        \"#####\",\n","        \"# <G#\",\n","        \"#####\"])\n","    assert str(om.sample(maze)) == ' #G#'\n","\n","    assert om.probability(om.sample(maze), maze) == 1.\n","\n","test_observation()"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"VdhLPT1srh6R"},"outputs":[{"name":"stdout","output_type":"stream","text":["# ##\n"]}],"source":["# Example\n","om = ObservationModel()\n","maze = State([\n","    \"####\",\n","    \"#^ #\",\n","    \"####\"])\n","print(om.sample(maze))"]},{"cell_type":"markdown","metadata":{"id":"NwVXtWK2rh6S"},"source":["## Example - Solve POMDPs using a solver\n","\n","- We will reuse models we defined before.\n","- Agent is initialized with a belief distributed uniformly over all the states."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"aHEZaWMnJpAw"},"outputs":[],"source":["def solve(\n","    init_true_state: PomdpState, \n","    steps: int = 15, \n","    planning_time: float = 0.5, \n","    max_depth: int = 10\n",") -> None:\n","    states = get_all_states(init_true_state)\n","    uniform_belief = {s: 1.0 / len(states) for s in states}\n","    init_belief = pomdp_py.Histogram(uniform_belief)\n","\n","    # 1. Select Agent, Environment\n","\n","    # Initialize agent, environment\n","    agent = pomdp_py.Agent(\n","        init_belief, \n","        PolicyModel(), \n","        TransitionModel(), \n","        ObservationModel(), \n","        RewardModel()\n","    )\n","    env = pomdp_py.Environment(\n","        init_true_state, TransitionModel(), RewardModel()\n","    )\n","\n","    # Initialize planners from library\n","    planner = pomdp_py.POUCT(\n","        max_depth=max_depth,\n","        discount_factor=0.95,\n","        planning_time=planning_time,\n","        exploration_const=110,\n","        rollout_policy=agent.policy_model,\n","    )\n","\n","    for i in range(steps):\n","        # 2. Agent plans an action at.\n","        print(\"\\n####################################\")\n","        print(f\"STEP {i}\")\n","        print(\"####################################\\n\")\n","        action = planner.plan(agent)\n","        print(f\"True state: \\n{env.state}\\n\")\n","        max_belief = agent.cur_belief.argmax()\n","        max_belief_prob = agent.cur_belief[max_belief]\n","        print(f\"Top Belief (Pr.= {round(max_belief_prob, 4)}) \\n{max_belief}\")\n","        print(f\"Action: {str(action)}\")\n","\n","        # 3. Environment state transitions \n","        # s_t -> s_t+1 according to its transition model.\n","        reward = env.state_transition(action, execute=True)\n","\n","        # 4. Agent receives an observation ot and reward \n","        # rt from the environment.\n","        observation = agent.observation_model.sample(env.state, action)\n","        print(f'Observation: \"{observation}\"')\n","        print(f\"Reward: {round(reward, 4)}\")\n","\n","        # 5.Agent updates history and belief\n","        agent.update_history(action, observation)\n","        planner.update(agent, action, observation)\n","        new_belief = pomdp_py.update_histogram_belief(\n","            agent.cur_belief,\n","            action,\n","            observation,\n","            agent.observation_model,\n","            agent.transition_model,\n","        )\n","        agent.set_belief(new_belief)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"wo6x0Daxrh6S"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","####################################\n","STEP 0\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","#^#  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.0052) \n","######\n","# #  #\n","# # G#\n","#^#  #\n","#   G#\n","######\n","Action: m\n","Observation: \" # #\"\n","Reward: -0.1\n","\n","####################################\n","STEP 1\n","####################################\n","\n","True state: \n","######\n","# #  #\n","#^# G#\n","# #  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.0417) \n","######\n","# #  #\n","# # G#\n","#^#  #\n","#   G#\n","######\n","Action: m\n","Observation: \"## #\"\n","Reward: -0.1\n","\n","####################################\n","STEP 2\n","####################################\n","\n","True state: \n","######\n","#^#  #\n","# # G#\n","# #  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","#^#  #\n","# #  #\n","# #  #\n","#    #\n","######\n","Action: l\n","Observation: \"### \"\n","Reward: -0.1\n","\n","####################################\n","STEP 3\n","####################################\n","\n","True state: \n","######\n","#<#  #\n","# # G#\n","# #  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","#<#  #\n","# #  #\n","# #  #\n","#    #\n","######\n","Action: m\n","Observation: \"### \"\n","Reward: -0.1\n","\n","####################################\n","STEP 4\n","####################################\n","\n","True state: \n","######\n","#<#  #\n","# # G#\n","# #  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","#<#  #\n","# #  #\n","# #  #\n","#    #\n","######\n","Action: l\n","Observation: \" ###\"\n","Reward: -0.1\n","\n","####################################\n","STEP 5\n","####################################\n","\n","True state: \n","######\n","#v#  #\n","# # G#\n","# #  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","#v#  #\n","# #  #\n","# #  #\n","#    #\n","######\n","Action: l\n","Observation: \"# ##\"\n","Reward: -0.1\n","\n","####################################\n","STEP 6\n","####################################\n","\n","True state: \n","######\n","#>#  #\n","# # G#\n","# #  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","#>#  #\n","# #  #\n","# #  #\n","#    #\n","######\n","Action: r\n","Observation: \" ###\"\n","Reward: -0.1\n","\n","####################################\n","STEP 7\n","####################################\n","\n","True state: \n","######\n","#v#  #\n","# # G#\n","# #  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","#v#  #\n","# #  #\n","# #  #\n","#    #\n","######\n","Action: l\n","Observation: \"# ##\"\n","Reward: -0.1\n","\n","####################################\n","STEP 8\n","####################################\n","\n","True state: \n","######\n","#>#  #\n","# # G#\n","# #  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","#>#  #\n","# #  #\n","# #  #\n","#    #\n","######\n","Action: r\n","Observation: \" ###\"\n","Reward: -0.1\n","\n","####################################\n","STEP 9\n","####################################\n","\n","True state: \n","######\n","#v#  #\n","# # G#\n","# #  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","#v#  #\n","# #  #\n","# #  #\n","#    #\n","######\n","Action: m\n","Observation: \" # #\"\n","Reward: -0.1\n","\n","####################################\n","STEP 10\n","####################################\n","\n","True state: \n","######\n","# #  #\n","#v# G#\n","# #  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","# #  #\n","#v#  #\n","# #  #\n","#    #\n","######\n","Action: m\n","Observation: \" # #\"\n","Reward: -0.1\n","\n","####################################\n","STEP 11\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","#v#  #\n","#   G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","# #  #\n","# # G#\n","#v#  #\n","#   G#\n","######\n","Action: m\n","Observation: \"##  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 12\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","# #  #\n","#v  G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","# #  #\n","# # G#\n","# #  #\n","#v  G#\n","######\n","Action: l\n","Observation: \" ## \"\n","Reward: -0.1\n","\n","####################################\n","STEP 13\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","# #  #\n","#>  G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","# #  #\n","# # G#\n","# #  #\n","#>  G#\n","######\n","Action: m\n","Observation: \" # #\"\n","Reward: -0.1\n","\n","####################################\n","STEP 14\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","# #  #\n","# > G#\n","######\n","\n","Top Belief (Pr.= 0.25) \n","######\n","# #  #\n","# # G#\n","# #  #\n","# > G#\n","######\n","Action: m\n","Observation: \"G#  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 15\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","# #  #\n","#  >G#\n","######\n","\n","Top Belief (Pr.= 0.5) \n","######\n","# #  #\n","# # G#\n","# #  #\n","#  >G#\n","######\n","Action: m\n","Observation: \"##  \"\n","Reward: 1.0\n","\n","####################################\n","STEP 16\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","# #  #\n","#   >#\n","######\n","\n","Top Belief (Pr.= 0.5) \n","######\n","# #  #\n","# # G#\n","# #  #\n","#   >#\n","######\n","Action: l\n","Observation: \" ## \"\n","Reward: -0.1\n","\n","####################################\n","STEP 17\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","# #  #\n","#   ^#\n","######\n","\n","Top Belief (Pr.= 0.5) \n","######\n","# #  #\n","# # G#\n","# #  #\n","#   ^#\n","######\n","Action: m\n","Observation: \"G#  \"\n","Reward: -0.1\n","\n","####################################\n","STEP 18\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # G#\n","# # ^#\n","#    #\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","# #  #\n","# # G#\n","# # ^#\n","#    #\n","######\n","Action: m\n","Observation: \" #  \"\n","Reward: 1.0\n","\n","####################################\n","STEP 19\n","####################################\n","\n","True state: \n","######\n","# #  #\n","# # ^#\n","# #  #\n","#    #\n","######\n","\n","Top Belief (Pr.= 1.0) \n","######\n","# #  #\n","# # ^#\n","# #  #\n","#    #\n","######\n","Action: l\n","Observation: \"  # \"\n","Reward: -0.1\n"]}],"source":["maze = PomdpState([\n","    \"######\",\n","    \"# #  #\",\n","    \"# # G#\",\n","    \"#^#  #\",\n","    \"#   G#\",\n","    \"######\"])\n","\n","solve(maze, steps=20, planning_time=1.0, max_depth=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J-JKbtAyrh6S"},"outputs":[],"source":["# Start in the middle.\n","# It takes some time for agent get reliable belief.\n","\n","maze = PomdpState([\n","    \"######\",\n","    \"#    #\",\n","    \"# ^  #\",\n","    \"#   G#\",\n","    \"######\"])\n","solve(maze, steps=15, planning_time=0.5, max_depth=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cQXlbVoqrh6T"},"outputs":[],"source":["# Start near wall.\n","# Agent quickly gets reliable belief.\n","\n","maze = PomdpState([\n","    \"######\",\n","    \"#^   #\",\n","    \"#    #\",\n","    \"#   G#\",\n","    \"######\"])\n","solve(maze, steps=15, planning_time=0.5, max_depth=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LhewHa9HQoLA"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"VENV_DIR","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"28faef28e7f0745a706c61265858622dc1f2f4e731abb5496d55c478fc6bd5d9"}}},"nbformat":4,"nbformat_minor":0}
