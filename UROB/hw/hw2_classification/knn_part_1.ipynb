{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e09de614221f1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div align=\"center\">\n",
    "      <h1>Introduction to k-Nearest Neighbors</h1>\n",
    "      <hr>\n",
    "      <p>This activity serves as a review of the k-Nearest Neighbors (k-NN) classifier, which you have previously encountered in the KUI course. Its purpose is to refresh your understanding of fundamental concepts related to machine learning classification tasks.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7180793044b6e28a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T10:28:42.004815513Z",
     "start_time": "2023-09-30T10:28:41.236505943Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02664bd52f34d0e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataset and Classification Task\n",
    "\n",
    "In supervised machine learning, a typical dataset consists of two main components:\n",
    "\n",
    "1. **Features** - These are the characteristics that describe the data. Usually, a sample is represented as a vector of features, denoted as $\\boldsymbol{x} = (x_{1}, x_{2}, ..., x_{d})$. In this assignment, the number of features will be referred to as $d$.\n",
    "\n",
    "2. **Labels** - These are the values we aim to predict. In this assignment, we'll use $y$ to denote the labels.\n",
    "\n",
    "The role of the classifier is to take a feature vector $\\boldsymbol{x}$ and predict the corresponding label $y$. Mathematically, this can be represented as a function $f$ that maps features to labels:\n",
    "\n",
    "$$f: \\mathbb{R}^{d} \\rightarrow \\mathcal{Y}$$\n",
    "\n",
    "$$f(\\boldsymbol{x}) = y, \\quad \\boldsymbol{x} \\in \\mathbb{R}^{d}, \\quad y \\in \\mathcal{Y}$$\n",
    "\n",
    "Once you have your dataset with features and labels, it's crucial to divide it into distinct subsets for training, validation, and testing. These subsets serve specific purposes during the model development and evaluation process:\n",
    "\n",
    "1. **Training Set** - This is the largest portion of your dataset and is used to train the model. The model learns underlying patterns in the data by adjusting its parameters based on the training examples. A larger training set generally helps the model learn more accurate and generalizable patterns.\n",
    "\n",
    "2. **Validation Set** - This subset is used during training for hyperparameter tuning and model selection. Hyperparameters are settings that are not learned by the model during training, such as the learning rate or the number of hidden layers in a neural network. The model's performance on the validation set helps in selecting the best hyperparameters and prevents overfitting, where the model becomes too tailored to the training data and performs poorly on new, unseen data.\n",
    "\n",
    "3. **Test Set** - This set is entirely independent of the training and validation data. Its purpose is to evaluate the final model's performance after it has been trained and tuned. Using a separate test set provides an unbiased estimate of how well the model is likely to perform on new, unseen data. This step is essential for assessing the model's generalization capabilities.\n",
    "\n",
    "*__Note__: In some situations, the terms \"validation set\" and \"test set\" may be used interchangeably. For example, in online literature, the validation set is often referred to as the test set. Additionally, in certain research papers, there may not be a distinct test set, and model evaluation is performed solely on the validation set. However, for this assignment, we will treat the validation and testing sets as separate entities.*\n",
    "\n",
    "Typical ratios for splitting the dataset include allocating 70-80% for training, 10-15% for validation, and another 10-15% for testing. However, these ratios can vary depending on the dataset size and the specific problem you're addressing. In cases where data is limited, techniques like **cross-validation** can be applied to maximize the utility of available data.\n",
    "\n",
    "To summarize, the training set teaches the model, the validation set aids in fine-tuning hyperparameters, and the test set offers an unbiased evaluation of performance. Properly segmenting these subsets is essential for effective and fair development and assessment of machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea58e065b1a4d99",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### The Data We'll Use\n",
    "\n",
    "Now, we'll work with a simple dataset designed for a classification task. This dataset comprises three classes of points in a 2D feature space. These points are generated from three distinct Gaussian distributions. The objective is to train a classifier capable of predicting the class to which a new point belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc7bc03128a7400d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T10:28:46.650777333Z",
     "start_time": "2023-09-30T10:28:42.004241126Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Training data ----------------\n",
      "X_train shape: (1125, 2), y_train shape: (1125,)\n",
      "\n",
      "---------------- Validation data ----------------\n",
      "X_val shape: (187, 2), y_val shape: (187,)\n",
      "\n",
      "---------------- Testing data ----------------\n",
      "X_test shape: (188, 2), y_test shape: (188,)\n",
      "\n",
      "---------------- Dataset info ----------------\n",
      "Number of features: 2\n",
      "Number of classes: 3\n",
      "Number of samples in dataset: 1500\n",
      "Number of samples in training set: 1125, which is 75.00% of the dataset\n",
      "Number of samples in validation set: 187, which is 12.47% of the dataset\n",
      "Number of samples in testing set: 188, which is 12.53% of the dataset\n"
     ]
    }
   ],
   "source": [
    "# from utils.visualizer import Visualizer\n",
    "\n",
    "# Load the dataset\n",
    "dataset = np.load('data/datasets/linearly_separable.npz')\n",
    "\n",
    "X_train, y_train = dataset['X_train'], dataset['y_train']\n",
    "X_val, y_val = dataset['X_val'], dataset['y_val']\n",
    "X_test, y_test = dataset['X_test'], dataset['y_test']\n",
    "\n",
    "# Print the shapes of the data\n",
    "print('---------------- Training data ----------------')\n",
    "print(f'X_train shape: {X_train.shape}, y_train shape: {y_train.shape}')\n",
    "\n",
    "print('\\n---------------- Validation data ----------------')\n",
    "print(f'X_val shape: {X_val.shape}, y_val shape: {y_val.shape}')\n",
    "\n",
    "print('\\n---------------- Testing data ----------------')\n",
    "print(f'X_test shape: {X_test.shape}, y_test shape: {y_test.shape}')\n",
    "\n",
    "print('\\n---------------- Dataset info ----------------')\n",
    "print(f'Number of features: {X_train.shape[1]}')\n",
    "print(f'Number of classes: {len(np.unique(y_train))}')\n",
    "print(f'Number of samples in dataset: {len(X_train) + len(X_val) + len(X_test)}')\n",
    "print(f'Number of samples in training set: {len(X_train)}, '\n",
    "      f'which is {100 * len(X_train) / (len(X_train) + len(X_val) + len(X_test)):.2f}% of the dataset')\n",
    "print(f'Number of samples in validation set: {len(X_val)}, '\n",
    "      f'which is {100 * len(X_val) / (len(X_train) + len(X_val) + len(X_test)):.2f}% of the dataset')\n",
    "print(f'Number of samples in testing set: {len(X_test)}, '\n",
    "      f'which is {100 * len(X_test) / (len(X_train) + len(X_val) + len(X_test)):.2f}% of the dataset')\n",
    "\n",
    "# vis = Visualizer((X_train, y_train), (X_val, y_val), (X_test, y_test))\n",
    "# vis.show_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237679746cb83211",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Nearest Neighbors Classifier\n",
    "\n",
    "The Nearest Neighbors classifier is a straightforward algorithm that operates as follows:\n",
    "\n",
    "1. **Training** - The classifier essentially memorizes the training dataset. The training set is stored in the classifier and used for prediction.\n",
    "2. **Prediction** - When making predictions, the classifier identifies the closest sample in the training set to the given input and assigns the label of the closest sample to the input.\n",
    "\n",
    "There are various methods to determine the closest sample, but for this assignment, we'll utilize the Euclidean distance metric. The Euclidean distance between two samples, denoted as $\\boldsymbol{x}_{1}$ and $\\boldsymbol{x}_{2}$, is defined as:\n",
    "\n",
    "$$d(\\boldsymbol{x}_{1}, \\boldsymbol{x}_{2}) = \\sqrt{\\sum_{i=1}^{d} (x_{1i} - x_{2i})^{2}}$$\n",
    "\n",
    "Here, $\\boldsymbol{x}_{1}$ and $\\boldsymbol{x}_{2}$ represent two samples from the dataset.\n",
    "\n",
    "## k-Nearest Neighbors Classifier\n",
    "\n",
    "The k-Nearest Neighbors classifier is akin to the Nearest Neighbors classifier, with one key difference. In the k-Nearest Neighbors classifier, instead of considering just the single closest sample, we find the $k$ closest samples from the training set. The classifier then assigns the label that is most prevalent among these $k$ closest samples to the given input.\n",
    "\n",
    "To illustrate, here's an example of k-Nearest Neighbors classification with $k=3$ (the yellow point represents the point we want to classify):\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "      <img src=\"data/images/knn_principle.png\" alt=\"k-Nearest Neighbors principle\">\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "In this example, the two nearest neighbors belong to the green class, while one belongs to the red class. Consequently, the yellow point is classified as part of the green class.\n",
    "\n",
    "Your task involves implementing the k-Nearest Neighbors classifier. This classifier should be capable of training on the training dataset and predicting labels for given samples. The implementation should reside in the `assignments/knn_classifier.py` file, which contains the `KNNClassifier` class with the following methods:\n",
    "\n",
    "1`_compute_distances(X)` - This method calculates the distances between the given samples and the training set.\n",
    "2`_compute_distances_vectorized(X)` - Here, distances between the given samples and the training set should be computed in a vectorized manner.\n",
    "3`_predict_labels(dists)` - This method predicts the labels of the given samples.\n",
    "\n",
    "Begin by implementing the `_compute_distances(X)` method, followed by the `_predict_labels(dists)` method. Once all methods are implemented, you can execute the provided code to train the classifier and predict labels for the given samples. Remember that, you can run tests to verify the correctness of your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3fa4da02ff641100",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T13:36:19.306011175Z",
     "start_time": "2023-09-22T13:36:17.834134782Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.984\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from assignments.knn_classifier import KNNClassifier\n",
    "\n",
    "# Create and train the classifier\n",
    "knn = KNNClassifier(k=3, vectorized=False)\n",
    "knn.train(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the given samples\n",
    "y_pred = knn.predict(X_val)\n",
    "\n",
    "# Compute the accuracy of the classifier, the accuracy should be around 0.98\n",
    "print(f'Accuracy: {accuracy_score(y_val, y_pred):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465f345586cfff6d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Vectorization\n",
    "\n",
    "Vectorization is a technique that can enhance the speed of your code by performing multiple operations simultaneously. In Python, we achieve vectorization through the use of the NumPy library. NumPy is a Python library that offers a multidimensional array object, various related objects (including masked arrays and matrices), and a range of functions for swift operations on arrays. These operations encompass mathematical computations, logical evaluations, shape manipulation, sorting, selection, input/output operations, discrete Fourier transforms, fundamental linear algebra operations, basic statistical computations, random simulations, and much more.\n",
    "\n",
    "In this assignment, you will apply vectorization to implement the k-Nearest Neighbors classifier efficiently. A vectorized approach is expected to be faster than a non-vectorized one. To implement the vectorized version, focus on the `_compute_distances_vectorized(X)` method within the `assignments/knn_classifier.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8e72f02791cc09d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T11:56:13.158197389Z",
     "start_time": "2023-09-20T11:56:13.090308066Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.984\n"
     ]
    }
   ],
   "source": [
    "knn_vectorized = KNNClassifier(k=3, vectorized=True)\n",
    "knn_vectorized.train(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the given samples\n",
    "y_pred = knn_vectorized.predict(X_val)\n",
    "\n",
    "# Compute the accuracy of the classifier. You can check that the accuracy is the same as for the non-vectorized implementation.\n",
    "print(f'Accuracy: {accuracy_score(y_val, y_pred):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a8af7a21a88959",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Non-vectorized vs. Vectorized Implementation\n",
    "\n",
    "Now, let's examine and compare the execution times of both the non-vectorized and vectorized implementations. We can assess the performance of these implementations using the `measure_time(classifier, X)` function. This function quantifies the time taken by the classifier to process a given dataset and returns the execution time in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52459735720e6b58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T11:56:13.897798240Z",
     "start_time": "2023-09-20T11:56:13.140396842Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-vectorized implementation took 1.145 seconds\n",
      "Vectorized implementation took 0.019 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "def measure_time(classifier: KNNClassifier, X: np.ndarray) -> float:\n",
    "    \"\"\" Measures the running time of predicting the labels of the given samples.\n",
    "    \n",
    "    Args:\n",
    "        classifier: The classifier to measure the running time of.\n",
    "        X: The dataset to measure the running time on.\n",
    "    \n",
    "    Returns:\n",
    "        The running time of the classifier in seconds.\n",
    "    \"\"\"\n",
    "\n",
    "    start = time()\n",
    "    _ = classifier.predict(X)\n",
    "    end = time()\n",
    "\n",
    "    return end - start\n",
    "\n",
    "\n",
    "print(f'Non-vectorized implementation took {measure_time(knn, X_val):.3f} seconds')\n",
    "print(f'Vectorized implementation took {measure_time(knn_vectorized, X_val):.3f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a9b39d5c21746d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div align=\"center\">\n",
    "      <hr>\n",
    "</div>\n",
    "\n",
    "### QUESTIONS:\n",
    "\n",
    "1. Why are NumPy operations faster than the same operations implemented in Python?\n",
    "2. In general, is vectorized implementation faster than implementation with loops on a CPU? (Hint: Can a CPU perform multiple operations at once?)\n",
    "3. Are there any disadvantages to vectorized implementation? If so, what are they?\n",
    "\n",
    "### ANSWERS:\n",
    "1. Because NumPy was implemented in low-level programming language (C, Fortran), that faster then Python.\n",
    "2. Yes, because of vectorized data processing in CPU.\n",
    "3. It's harder to understand and debug.\n",
    "\n",
    "<div align=\"center\">\n",
    "      <hr>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb896d444c6e98bc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Decision Boundaries\n",
    "\n",
    "Now let's visualize the decision boundaries of the k-Nearest Neighbors classifier. The decision boundaries are the boundaries in $d$-dimensional space that separate the samples from different classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f627248299d8012",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T11:56:19.858637149Z",
     "start_time": "2023-09-20T11:56:13.883347185Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the decision boundaries of the k-Nearest Neighbors classifier\n",
    "#vis.show_decision_boundaries(knn_vectorized, h=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c58e4efd5d1cd0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Evaluation of the Classifier\n",
    "\n",
    "To conclude our notebook, let's assess the performance of the k-Nearest Neighbors classifier. We can determine the classifier's effectiveness by employing the accuracy metric on the testing set. (In case you are unfamiliar with the accuracy metric, look it up.) This evaluation step provides valuable insights into how well our classifier generalizes to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4791d8852ebf0bf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T11:56:37.161919294Z",
     "start_time": "2023-09-20T11:56:36.135119322Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
