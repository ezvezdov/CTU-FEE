{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T17:18:28.661207306Z",
     "start_time": "2024-01-02T17:18:22.291810703Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvironment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPendulumEnv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PendulumEnv, base_config\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_training, plot_action_vs_angle\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/UROB/hw/hw4/venv/lib/python3.10/site-packages/torch/__init__.py:1750\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m library\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTORCH_CUDA_SANITIZER\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n",
      "File \u001b[0;32m~/Programming/UROB/hw/hw4/venv/lib/python3.10/site-packages/torch/_meta_registrations.py:5716\u001b[0m\n\u001b[1;32m   5712\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5713\u001b[0m                 _meta_lib_dont_use_me_use_register_meta\u001b[38;5;241m.\u001b[39mimpl(op_overload, fn)\n\u001b[0;32m-> 5716\u001b[0m \u001b[43mactivate_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/UROB/hw/hw4/venv/lib/python3.10/site-packages/torch/_meta_registrations.py:5713\u001b[0m, in \u001b[0;36mactivate_meta\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5711\u001b[0m     _meta_lib_dont_use_me_use_register_meta_for_onednn\u001b[38;5;241m.\u001b[39mimpl(op_overload, fn)\n\u001b[1;32m   5712\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 5713\u001b[0m     \u001b[43m_meta_lib_dont_use_me_use_register_meta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_overload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/UROB/hw/hw4/venv/lib/python3.10/site-packages/torch/library.py:141\u001b[0m, in \u001b[0;36mLibrary.impl\u001b[0;34m(self, op_name, fn, dispatch_key)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_dispatch_has_kernel_for_dispatch_key(dispatcher_op_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompositeImplicitAutograd\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    136\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe should not register a meta kernel directly to the operator \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m because it has a CompositeImplicitAutograd kernel in core.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Instead we should let the operator decompose, and ensure that we have meta kernels\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for the base ops that it decomposes into.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispatch_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdispatch_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCompositeImplicitAutograd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m _impls\u001b[38;5;241m.\u001b[39madd(key)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op_impls\u001b[38;5;241m.\u001b[39madd(key)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:230\u001b[0m, in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:132\u001b[0m, in \u001b[0;36mrelease\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from environment.PendulumEnv import PendulumEnv, base_config\n",
    "import torch\n",
    "from utils.plotting import plot_training, plot_action_vs_angle\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca602ce22817acd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction\n",
    "## Problem definition\n",
    "Our objective is to design a controller for the classic inverted pendulum (https://en.wikipedia.org/wiki/Inverted_pendulum) using reinforcement learning (RL) techniques. This system consists of a cart with a pendulum on top of it attached via a rotational joint. The cart can only move in one dimension and we are only allowed to apply a force onto the cart. The goal then is to stabilize the pendulum in the upright position.\n",
    "\n",
    "![Inverted pendulum](https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Cart-pendulum.svg/436px-Cart-pendulum.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618617296a796828",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## System model\n",
    "\n",
    "Although the system we wish to control is governed by continuous-time dynamics (i.e. a differential equation in the form $\\dot{x} = f(x)$), the control algorithm typically operates in discrete time. This justifies the following conceptual model for this problem.\n",
    "\n",
    "$$\n",
    "s_{t+1} = f(s_t, a_t)\\\\\n",
    "r_t = r(s_t, a_t)\n",
    "$$\n",
    "\n",
    "where $s_t \\in \\mathcal{S}$ is the state of the system at time $t$ (here $\\mathcal{S} = \\mathbb{R}^4$), $a_t \\in \\mathcal{A}$ is the action (applied force) at time $t$ (here $\\mathcal{A} = [-F_{max}, F_{max}]$) and $r_t \\in \\mathbb{R}$ is a scalar reward gained at timestep $t$. $f$ maps the current state $s_t$ and action $a_t$ into the next state $s_{t+1}$ and describes the discretized dynamics of the system. The reward function $r$ should in some way reflect our original objective - to stabilize the pendulum in the upright position. \n",
    "\n",
    "Our goal is then to find a policy (a control rule / controller) $\\pi : \\mathcal{S} \\rightarrow \\mathcal{A}$, which maximizes the accumulated reward over interactions (episodes) with the system.\n",
    "$$\n",
    "\\max_\\pi \\sum_{t=0}^{T-1} r_t\n",
    "$$\n",
    "where $T$ is the episode length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e58eb145002c53e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Environment\n",
    "\n",
    "Reinforcement learning uses interactions with the system to learn a control policy maximizing the accumulated reward. Although it is possible to learn by interactions with the real physical system, it is common to train on a simulated version of it instead. This however means the applicability of the learned policy depends on how accurate the simulation is to the real system.\n",
    "\n",
    "To implement the environment model, we will be using the mujoco (https://mujoco.org/) physical simulator. This simulator is given a simple model of the inverted pendulum described above and approximates the behavior of the real system using numerical integration.\n",
    "\n",
    "The underlying simulator is wrapped in the `PendulumEnv` class. We start by copying the default configuration dictionary `base_config`. This dictionary contains the field `N` which sets the number of simulated pendulums in the environment. The `vis` field toggles rendering of the simulated environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad70575ba31260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T17:18:28.840269257Z",
     "start_time": "2024-01-02T17:18:28.668198602Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(base_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a088fc1c2cb89cd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The `PendulumEnv` constructor takes in the configuration dictionary and creates mujoco simulated environment in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a6cfcd25b18c3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.672274633Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = PendulumEnv(base_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd076c49f829a9b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The function `vector_reset()` is used to reset the internal simulation state to it's initial value and returns the initial state of system $s_0 \\in \\mathbb{R}^4$. To interact with the environment, we use the environment's `vector_step()` function. This function takes an array of actions $a_t \\in [-1,1]$ (the actions are normalized for convenience), performs a simulation step $s_{t+1} = f(s_t, a_t)$ and returns the observed states $s_{t+1} \\in \\mathbb{R}^4$ and rewards $r_t \\in \\mathbb{R}$. The actions, states and rewards are all stored in arrays of length equal to the number of pendulums in the environment. \n",
    "\n",
    "Learning a control policy using RL requires a large amount of interactions with the controlled environment. Simulating multiple pendulums inside one simulation instance allows doing this more efficiently.\n",
    "\n",
    "Below is an example of environment interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4a15ddb7442bbf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.678685260Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = 512\n",
    "\n",
    "s0 = env.vector_reset()\n",
    "print('Returned state shape: ', np.array(s0).shape)\n",
    "\n",
    "for i in range(T):\n",
    "    actions = np.random.rand(base_config['N'], 1)*2 - 1  # random actions sampled from [-1, 1]\n",
    "    s, r = env.vector_step(actions)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a91367e49c8913",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Policy gradient method\n",
    "Though there are many approaches, which fall under the term reinforcement learning, we will focus on optimizing the policy directly. First, we need to construct a parametrized stochastic policy\n",
    "$$\n",
    "\\pi_{\\theta}(a_t | s_t)\n",
    "$$\n",
    "which maps the observed state $s_t$ into a probability distribution over the possible actions $a_t \\in \\mathcal{A}$. The policy parameters $\\theta$ are what we are going to optimize over (think of them as neural network weights).\n",
    "\n",
    "Let us denote the trajectory return (accumulated reward over a trajectory) as \n",
    "$$\n",
    "R(\\tau) = \\sum_{t=0}^{T-1} r_t\n",
    "$$\n",
    "where $\\tau = (s_0, a_0, s_1, a_1, ... , s_{T-1}, a_{T-1}, s_{T})$ is a given trajectory. We then solve the following optimization task\n",
    "$$\n",
    "\\theta = \\arg \\max_{\\theta} \\mathbb{E}_{\\tau} \\left[ R(\\tau) \\middle| \\theta \\right]\n",
    "$$\n",
    "using gradient ascent. The objective is simply the expected return of an episode conditioned on the policy parameters $\\theta$\n",
    "$$\n",
    "\\mathbb{E}_{\\tau} \\left[ R(\\tau) \\middle| \\theta \\right] = \\int_\\tau R(\\tau) p(\\tau | \\theta)\n",
    "$$\n",
    "where $p(\\tau | \\theta)$ is the probability of trajectory $\\tau$ conditioned on $\\theta$. This has a simple intuitive interpretation: find the policy parameters $\\theta$, such that the probability of trajectories with high return is maximized. \n",
    "\n",
    "Even though the resulting policy will be used in deterministic manner, the probabilistic formulation is necessary for two reasons. The stochasticity allows us to explore the action space during training and it also allows us to compute the gradient of the objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83555a7f3ae82c57",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## <font color='blue'> OPTIONAL READING </font>\n",
    "## Deriving the policy gradient\n",
    "The maximization objective is\n",
    "$$\n",
    "J = \\int_\\tau p(\\tau | \\theta) R(\\tau) d\\tau\n",
    "$$\n",
    "The probability of the trajectory is given by\n",
    "$$\n",
    "p(\\tau | \\theta) = p(s_0)\\pi_\\theta(a_0 | s_0)p(s_1 | s_0, a_0) ... p(s_{T-1} | s_{T-2}, a_{T-2})\\pi_\\theta(a_{T-1} | s_{T-1})p(s_{T}|s_{T-1}, a_{T-1})\n",
    "$$\n",
    "where $p(s_0)$ is the probability distribution of the initial state, $\\pi_\\theta$ is the policy and $p(s_{t+1} | s_t, a_t)$ here just represents the system dynamics. We assume the system dynamics to be deterministic and so the corresponding distribution is formally\n",
    "$$\n",
    "p(s_{t+1} | s_t, a_t) = \\delta(s_{t+1} - f(s_t, a_t))\n",
    "$$\n",
    "i.e. zero everywhere except for the next state $f(s_t, a_t)$. In the general case, the integral in the objective function cannot be computed analytically (unless we assume, say, linear deterministic controller and quadratic reward function). Numerical integration would suffer from the curse of dimensionality, since the length of the episodes will be in the hundreds in our case (which would lead to integrating over hundreds of variables in $\\tau$). \n",
    "Luckily, we can get around this problem by applying the 'log trick' to the gradient of our objective\n",
    "$$\n",
    "\\nabla_\\theta J = \\nabla_\\theta \\int_\\tau p(\\tau | \\theta) R(\\tau) d\\tau = \\int_\\tau \\nabla_\\theta p(\\tau | \\theta) R(\\tau) d\\tau = \\int_\\tau p(\\tau | \\theta) \\nabla_\\theta \\log (p(\\tau | \\theta)) R(\\tau) d\\tau\n",
    "$$\n",
    "where we used the fact that $\\nabla_\\theta \\log(p(\\tau | \\theta)) =  \\frac{\\nabla_\\theta p(\\tau | \\theta)}{p(\\tau | \\theta)}$. This enables us to <b>estimate the policy gradient</b> by sampling trajectories using current policy\n",
    "$$\n",
    "\\nabla_\\theta J \\approx \\hat{g} = \\dfrac{1}{N} \\sum_{i=1}^N \\nabla_\\theta \\log (p(\\tau_i | \\theta)) R(\\tau_i)\n",
    "$$\n",
    "where $\\tau_i$ are the sampled trajectories. These trajectories $\\tau_i$ are obtained by simply interacting with the environment while choosing actions according to the policy. To be clear, we are actually approximating the true trajectory distribution $p(\\tau | \\theta)$ using sampled trajectories as \n",
    "$$\n",
    "p(\\tau | \\theta) \\approx \\hat{p}(\\tau | \\theta) = \\dfrac{1}{N} \\sum_{i=1}^N \\delta(\\tau - \\tau_i)\n",
    "$$\n",
    "Now we can use this 'policy gradient' for gradient ascent to iteratively improve our policy, hopefully increasing the expected accumulated reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee37613ad12ab8b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Simplifying the policy gradient\n",
    "\n",
    "The formula for the approximated policy gradient can be further simplified by using the log-product rule \n",
    "$$\n",
    "\\nabla_\\theta J \\approx \\dfrac{1}{N} \\sum_{i=1}^N \\nabla_\\theta \\log (p(\\tau_i | \\theta)) R(\\tau_i) \\propto \\dfrac{1}{N}\\dfrac{1}{T} \\sum_{i=1}^N R(\\tau_i) \\sum_{t=0}^{T-1} \\nabla_\\theta \\log (\\pi_\\theta(a^i_t | s^i_t)) \n",
    "$$\n",
    "where $s^i_t, a^i_t$ denote the state and action belonging to the $i$-th trajectory. Note that we also normalize the gradient by dividing with episode length $T$. So the finalized formula for policy gradient that we will use for policy training is\n",
    "$$\n",
    "\\textcolor{red}{\\hat{g} = \\dfrac{1}{N}\\dfrac{1}{T} \\sum_{i=1}^N R(\\tau_i) \\sum_{t=0}^{T-1} \\nabla_\\theta \\log (\\pi_\\theta(a^i_t | s^i_t))}\n",
    "$$\n",
    "This formula will be useful for your first task.\n",
    "\n",
    "### Training algorithm\n",
    "Now we can use this policy gradient approximation to train the policy according to the following pseudo-algorithm:\n",
    "\n",
    "Given policy parameters $\\theta_0$, and learning rate $\\alpha > 0$ \n",
    "For k in range(iters):\n",
    "1) Collect N trajectories $\\{\\tau_i \\}_{i=1}^N$, while using the current policy parameters $\\theta_k$ for action sampling.\n",
    "2) For each trajectory, compute the return $R(\\tau_i) = \\sum_{t=0}^{T-1} r^i_t$\n",
    "3) Compute the policy gradient approximation $\\hat{g}$ using the returns and log-probabilities\n",
    "4) Update the policy $\\theta_{k+1} = \\theta_{k} + \\alpha \\hat{g}$\n",
    "\n",
    "Note: when using autodiff engine such as pytorch, we need to compute the <b>loss</b> instead of the policy gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b98fcf89e4092fe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Policy\n",
    "\n",
    "We talked about how we can train a stochastic policy $\\pi_\\theta(a_t | s_t)$, but how do we actually implement it? Since this is a deep learning course, our policy will be represented by a neural network. The neural network $n_{\\theta}(s_t)$ is however a deterministic mapping on its own. A common way to solve this is to create the distribution from the neural network outputs. But this must be done, so that we can back-propagate through this distribution afterward. \n",
    "\n",
    "One possible solution is to make the network output mean and standard deviation $(\\mu_\\theta(s_t), \\sigma_\\theta(s_t))$ of a normal distribution. The actions are then sampled from the resulting normal distribution \n",
    "$$\n",
    "a_t \\sim N(\\mu_\\theta(s_t), \\sigma_\\theta(s_t))\n",
    "$$\n",
    "And in this way, we have created the parametrized stochastic policy that we will train!\n",
    "$$\n",
    "\\pi_\\theta(a_t | s_t) = N(\\mu_\\theta(s_t), \\sigma_\\theta(s_t))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425c0d5de06712f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Implementation \n",
    "\n",
    "## Policy\n",
    "We start by implementing our parametrized stochastic policy using pytorch. This part is already done below in the form of `StochasticPolicy` class. It initializes a neural network that maps from the state space $\\mathbb{S} = \\mathbb{R}^4$ into the normal distribution mean and standard deviation $\\mu, \\sigma$ as described above.\n",
    "\n",
    "The `determine_actions()` function returns only the mean $\\mu$ obtained computed by the neural network and can be used to sample deterministic actions. \n",
    "\n",
    "The `sample_actions()` function implements sampling from the normal distribution constructed from the network outputs and can be used during training for action space exploration.\n",
    "\n",
    "The `log_prob()` function returns the logarithm of action probabilities $\\log ( \\pi_\\theta(a_t | s_t) )$ given the observed states. This can be used for computing the policy gradient approximation for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60049e23a2295f71",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.685851551Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "class StochasticPolicy(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim=4, action_dim=1):\n",
    "        super(StochasticPolicy, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 2*action_dim),\n",
    "        )\n",
    "        \n",
    "    def determine_actions(self, states):\n",
    "        \"\"\"states is (N, state_dim) tensor, returns (N, action_dim) actions tensor. \n",
    "        This function returns deterministic actions based on the input states. This would be used for control. \"\"\"\n",
    "        params = self.network(states)  # map states to distribution parameters\n",
    "        mu, _ = torch.chunk(params, 2, -1)  # split the parameters into mean and std, return mean\n",
    "        return mu\n",
    "        \n",
    "    def sample_actions(self, states):\n",
    "        \"\"\"states is (T, N, state_dim) tensor, returns (T, N, action_dim) actions tensor. \n",
    "        This function returns probabilistically sampled actions. This would be used for training the policy.\"\"\"\n",
    "        params = self.network(states)  # map states to distribution parameters\n",
    "        mu, sigma = torch.chunk(params, 2, -1)  # split the parameters into mean and std\n",
    "        sigma = torch.nn.functional.softplus(sigma)  # make sure std is positive\n",
    "        distribution = Normal(mu, sigma)  # create distribution of size (T, N, action_dim)\n",
    "        actions = distribution.sample()  # sample actions\n",
    "        return actions\n",
    "    \n",
    "    def log_prob(self, actions, states):\n",
    "        \"\"\"states is (T, N, state_dim) tensor. actions is (T, N, action_dim) tensor.\n",
    "        This function returns the log-probabilities of the actions given the states. $\\log \\pi_\\theta(a_t | s_t)$\"\"\"\n",
    "        params = self.network(states)  # map states to distribution parameters\n",
    "        mu, sigma = torch.chunk(params, 2, -1)  # split the parameters into mean and std\n",
    "        sigma = torch.nn.functional.softplus(sigma)  # make sure std is positive\n",
    "        distribution = Normal(mu, sigma)  # create distribution of size (T, N, action_dim)\n",
    "        logp = distribution.log_prob(actions)\n",
    "        if len(logp.shape) == 3 and logp.shape[2] > 1:  # this allows generalization to multi-dim action spaces\n",
    "            logp = logp.sum(dim=2, keepdim=True)  # sum over the action dimension\n",
    "        return logp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f575d1c3c9508",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Policy controlling the environment example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67fb578b422ee9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.691808165Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_policy(pi, T=128, config=base_config, deterministic=True):\n",
    "    test_env = PendulumEnv(config)\n",
    "    mean_reward = 0\n",
    "    \n",
    "    s = test_env.vector_reset()\n",
    "    for i in range(T):\n",
    "        with torch.no_grad():\n",
    "            if deterministic:\n",
    "                actions = pi.determine_actions(torch.tensor(s).float())  # use deterministic actions based on the states\n",
    "            else:\n",
    "                actions = pi.sample_actions(torch.tensor(s).float())  # use random actions conditioned on the states\n",
    "        s, r = test_env.vector_step(actions.numpy())\n",
    "        mean_reward += sum(r)/(T*config['N'])\n",
    "        \n",
    "    test_env.close()\n",
    "    return mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1938e94aee88e0bb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.698464782Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = {'N': 64, 'vis':True}\n",
    "\n",
    "pi = StochasticPolicy(state_dim=4)\n",
    "\n",
    "test_policy(pi, 256, config, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f23e30db0c8f0b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.704385994Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_action_vs_angle(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ac3e411379a0b4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 1) Simple policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7ac2cff4eb0e2a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.710443337Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: complete the implementation in solution.py\n",
    "from solution import policy_gradient_loss_simple  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c42773d9c1bb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Quick test for your implementation is below. The output should be: tensor(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a765ca50437d5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.716972259Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_r = torch.tensor([1, 2, 1, 0, 1, 1, 0, 2]).reshape((8, 1)).float()  # testing rewards \n",
    "test_logp = torch.tensor([0, -3, 0, -1, -2, 3, 1, 1]).reshape((8, 1)).float()  # testing logp\n",
    "\n",
    "print(policy_gradient_loss_simple(test_logp, test_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6df3972801451",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Helper function for collecting trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab4ef54d639b52",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.723076424Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_trajectories(env, pi, T):    \n",
    "    \"\"\"given an environment env, a stochastic policy pi and number of timesteps T, interact with the environment for T steps \n",
    "    using actions sampled from policy. Return torch tensors of collected states, actions and rewards\"\"\"\n",
    "    states = np.zeros((T + 1, N, env.num_states), dtype=float)  # states from s(0) to s(T+1)\n",
    "    actions = np.zeros((T, N, env.num_actions), dtype=float)  # actions from a(0) to a(T)\n",
    "    rewards = np.zeros((T, N), dtype=float)  # rewards from r(0) to r(T)\n",
    "    \n",
    "    s = env.vector_reset()\n",
    "    states[0] = s\n",
    "    for t in range(T):\n",
    "        a = pi.sample_actions(torch.tensor(states[t]).float())  # policy needs float torch tensor (N, state_dim)\n",
    "        s_next, r = env.vector_step(np.array(a))  # env needs numpy array of (Nx1)\n",
    "        states[t + 1], actions[t], rewards[t] = s_next, a, r    \n",
    "        \n",
    "    tensor_s = torch.tensor(states).float()  # (T+1, N, state_dim)  care for the extra timestep at the end!\n",
    "    tensor_a = torch.tensor(actions).float()  # (T, N, 1)\n",
    "    tensor_r = torch.tensor(rewards).float()  # (T, N)\n",
    "    \n",
    "    return tensor_s, tensor_a, tensor_r "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d3eb53005cdb3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Time to train the policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c9332c0825b7c1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.728942373Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "N = 32\n",
    "T = 128\n",
    "config = {'N': N, 'vis': False}\n",
    "epochs = 100\n",
    "lr = 0.01\n",
    "\n",
    "# policy, environment and optimizer\n",
    "pi = StochasticPolicy(state_dim=4)\n",
    "train_env = PendulumEnv(config)\n",
    "optim = torch.optim.SGD(pi.parameters(), lr=lr)\n",
    "\n",
    "mean_rewards, p_losses = np.zeros(epochs), np.zeros(epochs)  # for logging mean rewards over epochs\n",
    "for epoch in range(epochs):\n",
    "    tensor_s, tensor_a, tensor_r = sample_trajectories(train_env, pi, T)  # collect trajectories using current policy\n",
    "    \n",
    "    logp = pi.log_prob(tensor_a, tensor_s[:T]).squeeze(2)  # compute log(pi(a_t | s_t))\n",
    "    loss = policy_gradient_loss_simple(logp, tensor_r)  # compute the policy gradient loss\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()  # backprop and gradient step\n",
    "    optim.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch %d, mean reward: %.3f' % (epoch, tensor_r.mean()))\n",
    "    mean_rewards[epoch] = tensor_r.mean()\n",
    "    p_losses[epoch] = loss.item()\n",
    "    \n",
    "train_env.close()\n",
    "\n",
    "plot_training(mean_rewards, p_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d1082c9a0f4bb3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.734960052Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_policy(pi, 256, {'N': 16, 'vis': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e560638530563210",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.745382069Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_action_vs_angle(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd00c6dce2960e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 2) Discounted policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877e1d08968f3d27",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reducing variance of the gradient\n",
    "\n",
    "As it turns out, using simple trajectory returns for policy gradient computation has quite poor performance. The policy gradient formula we derived is \n",
    "$$\n",
    "\\hat{g} = \\dfrac{1}{N}\\dfrac{1}{T} \\sum_{i=1}^N R(\\tau_i) \\sum_{t=0}^{T-1} \\nabla_\\theta \\log (\\pi_\\theta(a^i_t | s^i_t)) \n",
    "$$\n",
    "which uses the trajectory return $R(\\tau_i) = \\sum_{t=0}^{T-1}r^i_t$ to weight each trajectory's contribution to the overall gradient. It however only differentiates between better and worse trajectories, but does not take into account the influence of individual actions. This leads to the gradient estimate having high variance, which makes training unstable. We would instead like to reinforce only the actions which are directly responsible for the increase in reward. Determining which actions and to what extent are responsible for the obtained reward is known as the credit assignment problem. \n",
    "\n",
    "To reduce the variance of the policy gradient estimate, we introduce a discount factor $\\gamma \\in (0,1)$, which reduces the influence of rewards that are far in the future. We also reformulate the return to account for infinite time horizon, which is more mathematically convenient. The return of the trajectory is then computed as \n",
    "$$\n",
    "R(\\tau) = \\sum_{t=0}^{\\infty} \\gamma^{t} r_t\n",
    "$$\n",
    "Note that while the variance of the estimate is decreased, we introduce bias (the expected value of the gradient estimate $\\hat{g}$ is no longer equal to the policy gradient $\\nabla_\\theta J$). For the discount factor to make sense, this return should be computed for each timestep individually. This makes sure that the exponential decay starts at the reward corresponding to the current timestep. To be clear, the gradient estimate implementation with the discounted return becomes\n",
    "$$\n",
    "\\textcolor{red}{\n",
    "\\hat{g} = \\dfrac{1}{N}\\dfrac{1}{T} \\sum_{i=1}^N \\sum_{t=0}^{T-1} \\left( \\sum_{t'=t}^{T-1} \\gamma^{t'-t} r_{t'} \\right)\\nabla_\\theta \\log (\\pi_\\theta(a^i_t | s^i_t)) \n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac38791bdfd40ed",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.751443698Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: complete the implementation of policy_gradient_loss_discounted() and discount_cum_sum() in solution.py\n",
    "from solution import policy_gradient_loss_discounted "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256236c8e0981ad0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Quick test for your implementation is below. The output should be: tensor(1.7653)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002ad7d9d2d353e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.763233371Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_r = torch.tensor([1, 2, 1, 0, 1, 1, 0, 2]).reshape((8, 1)).float()  # testing rewards \n",
    "test_logp = torch.tensor([0, -3, 0, -1, -2, 3, 1, 1]).reshape((8, 1)).float()  # testing logp\n",
    "test_gamma=0.9\n",
    "\n",
    "print(policy_gradient_loss_discounted(test_logp, test_r, test_gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49503fa1bb2b9373",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.769140870Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "N = 32\n",
    "T = 128\n",
    "config = {'N': N, 'vis': 0}\n",
    "epochs = 100\n",
    "lr = 0.01\n",
    "gamma=0.95\n",
    "\n",
    "# policy, environment and optimizer\n",
    "pi = StochasticPolicy(state_dim=4)\n",
    "train_env = PendulumEnv(config)\n",
    "optim = torch.optim.SGD(pi.parameters(), lr=lr)\n",
    "\n",
    "mean_rewards, p_losses = np.zeros(epochs), np.zeros(epochs)\n",
    "for epoch in range(epochs):\n",
    "    tensor_s, tensor_a, tensor_r = sample_trajectories(train_env, pi, T)  # collect trajectories using current policy\n",
    "\n",
    "    logp = pi.log_prob(tensor_a, tensor_s[:T]).squeeze(2)  # compute log(pi(a_t | s_t))\n",
    "    loss = policy_gradient_loss_discounted(logp, tensor_r, gamma)  # compute the policy gradient loss\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()  # backprop and gradient step\n",
    "    optim.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch %d, mean reward: %.3f' % (epoch, tensor_r.mean()))\n",
    "    mean_rewards[epoch] = tensor_r.mean()\n",
    "    p_losses[epoch] = loss.item()\n",
    "    \n",
    "train_env.close()\n",
    "\n",
    "plot_training(mean_rewards, p_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b91b3907e54e82",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.773738014Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_policy(pi, 256, {'N': 16, 'vis': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ca0a6c93300ad",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.815330864Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_action_vs_angle(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b378ad45ac25e83d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Task 3) policy gradient estimate using advantage\n",
    "\n",
    "To further reduce variance of the estimate, we replace the discounted returns $R(\\tau)$ with so-called advantage function. \n",
    "$$\n",
    "A^{\\pi}(s,a) = Q^{\\pi}(s,a) - V^{\\pi}(s)\n",
    "$$\n",
    "where $V$ and $Q$ are called value function and action-value function respectively. Formally, \n",
    "$$\n",
    "V^{\\pi}(s) = \\mathbb{E}_{\\tau} \\left[ \\sum_{t=0}^{\\infty} \\gamma^{t} r_t \\middle| s_0 = s \\right] \\\\\n",
    "Q^{\\pi}(s,a) = \\mathbb{E}_{\\tau} \\left[ \\sum_{t=0}^{\\infty} \\gamma^{t} r_t \\middle| s_0 = s, a_0 = a \\right]\n",
    "$$\n",
    "\n",
    "Informally, the value function is the expected return of a trajectory starting in a given state $s$, while following the policy $\\pi$. The action-value function is the same thing, except it assumes we take the specific action $a$ from state $s$ and follow the policy onward. The advantage function $A^{\\pi}(s,a)$ then expresses, how the expected return improves, if we take the given action $a$ as compared to following the policy. \n",
    "If the advantage for a given action is positive, it means that taking the given action $a$ increases the expected return compared to the current policy, and so we should increase its probability. The only problem is that this advantage function is unknown to us, and so we have to estimate it. This makes the training a little more complicated, but pays off in reduced gradient estimate variance. \n",
    "\n",
    "### Advantage estimation overview\n",
    "Advantage function estimation is beyond the scope of this assignment, and so we will only go over the basic idea. First, we construct a value network \n",
    "$$\n",
    "\\hat{V}_\\theta : \\mathcal{S} \\rightarrow \\mathbb{R}\n",
    "$$\n",
    "that serves to approximates the true value function $V^{\\pi}(s)$. The $\\theta$ denotes the learnable parameters, which are usually shared with the policy network. The value function is easier to approximate than the advantage function, since it is only a function of the state. The advantage estimates $\\hat{A}(s, a)$ are then be computed using the value network outputs and collected rewards. \n",
    "\n",
    "This value network is trained in parallel with the policy to regress value targets $V^*(s)$, which are estimated from the trajectory rewards. This is typically done by minimizing the L2 distance between the value network and the value targets \n",
    "$$\n",
    "\\textcolor{red}{\n",
    "L_v(\\theta) = \\dfrac{1}{N}\\dfrac{1}{T} \\sum_{i=1}^N \\sum_{t=0}^{T-1}  \\left( \\hat{V}_\\theta(s_t^i) - V^*(s_t^i)\\right)^2\n",
    "}\n",
    "$$\n",
    "using gradient descent. The advantage estimation and value target computation is already implemented below.\n",
    "\n",
    "The resulting policy gradient estimate using the estimated advantages is then\n",
    "$$\n",
    "\\textcolor{red}{\n",
    "\\hat{g} = \\dfrac{1}{N}\\dfrac{1}{T} \\sum_{i=1}^N \\sum_{t=0}^{T-1} \\hat{A}(s_t^i, a_t^i)\\nabla_\\theta \\log (\\pi_\\theta(a^i_t | s^i_t)) \n",
    "}\n",
    "$$\n",
    "where $\\hat{A}(s, a)$ is the advantage estimate for given state $s$ and action $a$. Even though the advantage estimates are also a function of the network parameters $\\theta$, we consider them to be constant when computing the policy gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bb7b173b71ce4b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## <font color='blue'> OPTIONAL READING </font>\n",
    "### Advantage estimation implementation\n",
    "\n",
    "The function `compute_advantage_estimates()` is given a tensor of collected rewards and estimated values on sampled trajectories together with the discount factor $\\gamma$. It then computes the value function targets and advantage estimates. The value targets will be used to train our value function network and the advantage estimates will be used to estimate the policy gradient. Note that it uses the function `discount_cum_sum()`.\n",
    "\n",
    "While it is not a part of the assignment, we can briefly summarize, what the function does. \n",
    "\n",
    "If `bootstrap` is disabled, it simply uses the discounted return as the value function target, which is consistent with the finite horizon formulation of trajectory return. This leads to a problem: the value targets near the start of the trajectory will have more terms in the return sum than those at the end. This means the range of possible value targets is larger at the start of the trajectory than at the end. This can in turn lead to convergence problems.\n",
    "\n",
    "If `bootstrap` is enabled, it appends the estimated values at the end of the trajectory (at time $T$) to the rewards and then computes the discounted returns, which is consistent with the infinite horizon formulation. The last value target in this scenario would be $V^*(s_T) =  r_{T-1} + \\gamma \\hat{V}(s_T)$. If our value estimate $\\hat{V}$ is good (accurate), this will mitigate the problem that arises without bootstrapping. If the value estimate is bad (inaccurate), we introduce a source of noise into the value function training.\n",
    "\n",
    "Irrelevant of how we acquire the value targets, the advantage estimates are computed as \n",
    "$$\n",
    "\\hat{A}^\\pi(s_t,a_t) = V^*(s_t) - \\hat{V}(s_t)\n",
    "$$\n",
    "where $V^*(s_t)$ is the value target at state $s_t$ and $\\hat{V}(s_t)$ is the value estimate at $s_t$. Note that we do not model the $Q$ function explicitly. Instead, the value target $V^*(s_t)$ can be considered a $Q$ function estimate, since it depends on which action we took from state $s_t$. We call it the value target, because we use it for the value network training.\n",
    "\n",
    "The function `compute_gae()` implements a more advanced version of the advantage estimation algorithm based on the paper https://arxiv.org/abs/1506.02438. In addition to the rewards, values and discount factor $\\gamma$, it accepts another hyperparameter $\\lambda \\in [0, 1]$, which controls the trade-off between bias and variance of the advantage estimate. For $\\lambda = 1$, the function is equivalent to the `compute_advantage_estimates()` function with bootstrapping enabled. Feel free to experiment with different settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b3ef76de6f10b8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.815574994Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from solution import discount_cum_sum\n",
    "\n",
    "def compute_advantage_estimates(tensor_r, values, gamma, bootstrap=False):\n",
    "    \"\"\"given reward tensor (T, N), value estimates tensor (T+1, N) and gamma scalar\"\"\"\n",
    "    if bootstrap:  # use last value estimates as a return estimate\n",
    "        terminal_value_estimates = values[-1].unsqueeze(0)  # values of the last states (1, N)\n",
    "        rs_v = torch.cat((tensor_r, terminal_value_estimates), dim=0)\n",
    "        value_targets = discount_cum_sum(rs_v, gamma)[:-1]\n",
    "    else:\n",
    "        value_targets = discount_cum_sum(tensor_r, gamma)\n",
    "    advantages = value_targets - values[:-1]\n",
    "    return value_targets, advantages\n",
    "\n",
    "def compute_gae(tensor_r, values, gamma, lambda_):\n",
    "    \"\"\"generalized advantage estimation (GAE) implementation\"\"\"\n",
    "    delta_t = tensor_r + gamma * values[1:] - values[:-1]\n",
    "    advantages = discount_cum_sum(delta_t, gamma * lambda_)\n",
    "    value_targets = advantages + values[:-1]\n",
    "    return value_targets, advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cae3de7bdd8f26",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stochastic policy with value function\n",
    "\n",
    "Below is an implementation of a stochastic policy together with value network in the form of the `StochasticPolicyValue` class. The policy has shared layers that are used to compute features for the action and value networks. This can be thought of as a way to share information between the two networks and can improve the performance.\n",
    "\n",
    "The `determine_actions()`, `sample_actions()` and `log_prob()` functions work the same way as in the previously used `StochasticPolicy`. \n",
    "\n",
    "The additional `value_estimates()` function is used to estimate the value function on given states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa3adaa3be095f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.815741614Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class StochasticPolicyValue(nn.Module):\n",
    "    def __init__(self, state_dim=4, action_dim=1):\n",
    "        super(StochasticPolicyValue, self).__init__()\n",
    "        \n",
    "        # shared layers between the action and value heads\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, 32),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        # action head\n",
    "        self.action_layers = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 2*action_dim),\n",
    "        )\n",
    "\n",
    "        # value head\n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "        \n",
    "    def determine_actions(self, states):\n",
    "        \"\"\"states is (N, state_dim) tensor, returns (N, action_dim) actions tensor. \n",
    "        This function returns deterministic actions based on the input states. This would be used for control. \"\"\"\n",
    "        params = self.action_layers(self.shared_layers(states))  # map states to distribution parameters\n",
    "        mu, _ = torch.chunk(params, 2, -1)  # split the parameters into mean and std, return mean\n",
    "        return mu\n",
    "        \n",
    "    def sample_actions(self, states):\n",
    "        \"\"\"states is (T, N, state_dim) tensor, returns (T, N, action_dim) actions tensor. \n",
    "        This function returns probabilistically sampled actions. This would be used for training the policy.\"\"\"\n",
    "        params = self.action_layers(self.shared_layers(states))  # map states to distribution parameters\n",
    "        mu, sigma = torch.chunk(params, 2, -1)  # split the parameters into mean and std\n",
    "        sigma = torch.nn.functional.softplus(sigma)  # make sure std is positive\n",
    "        distribution = Normal(mu, sigma)  # create distribution of size (T, N, action_dim)\n",
    "        actions = distribution.sample()  # sample actions\n",
    "        return actions\n",
    "    \n",
    "    def log_prob(self, actions, states):\n",
    "        \"\"\"states is (T, N, state_dim) tensor. actions is (T, N, action_dim) tensor.\n",
    "        This function returns the log-probabilities of the actions given the states. $\\log \\pi_\\theta(a_t | s_t)$\"\"\"\n",
    "        params = self.action_layers(self.shared_layers(states))  # map states to distribution parameters\n",
    "        mu, sigma = torch.chunk(params, 2, -1)  # split the parameters into mean and std\n",
    "        sigma = torch.nn.functional.softplus(sigma)  # make sure std is positive\n",
    "        distribution = Normal(mu, sigma)  # create distribution of size (T, N, action_dim)\n",
    "        logp = distribution.log_prob(actions)\n",
    "        if len(logp.shape) == 3 and logp.shape[2] > 1:  # this allows generalization to multi-dim action spaces\n",
    "            logp = logp.sum(dim=2, keepdim=True)  # sum over the action dimension\n",
    "        return logp\n",
    "    \n",
    "    def value_estimates(self, states):\n",
    "        \"\"\"states is (T, N, state_dim) tensor, returns (T, N) values tensor. Useful for value estimation during training.\"\"\"\n",
    "        return self.value_layers(self.shared_layers(states)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1899e3b49b7bfb2a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.815890255Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: implement policy_gradient_loss_advantages and value_loss in solution.py\n",
    "from solution import policy_gradient_loss_advantages, value_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7415fb03b05df",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Quick test for your implementation is below. The output should be: tensor(1.6250) tensor(4.8750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c838f7b3cb024c6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.816042078Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_logp = torch.tensor([0, -3, 0, -1, -2, 3, 1, 1]).reshape((8, 1)).float()  # testing logp\n",
    "test_advantage_estimates = torch.tensor([0, 3, 3, -1, 2, 1, -2, -2]).reshape((8, 1)).float()  # testing rewards \n",
    "test_values = torch.tensor([1, 2, 1, 0, 1, 1, 0, 2]).reshape((8, 1)).float()  # testing rewards \n",
    "test_value_targets = torch.tensor([0, -2, 0, -1, 2, 0, -3, -1]).reshape((8, 1)).float()  # testing rewards \n",
    "\n",
    "print(policy_gradient_loss_advantages(test_logp, test_advantage_estimates), value_loss(test_values, test_value_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c3e32594ccd7ba",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.816193547Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "N = 32\n",
    "T = 128\n",
    "config = {'N': N, 'vis': 0}\n",
    "epochs = 100\n",
    "lr = 0.01\n",
    "gamma=0.95\n",
    "\n",
    "# policy, environment and optimizer\n",
    "pi = StochasticPolicyValue(state_dim=4)\n",
    "train_env = PendulumEnv(config)\n",
    "optim = torch.optim.SGD(pi.parameters(), lr=lr)\n",
    "\n",
    "mean_rewards, p_losses, v_losses = np.zeros(epochs), np.zeros(epochs), np.zeros(epochs)  # logging\n",
    "for epoch in range(epochs):\n",
    "    tensor_s, tensor_a, tensor_r = sample_trajectories(train_env, pi, T)  # collect trajectories using current policy\n",
    "    \n",
    "    values = pi.value_estimates(tensor_s)  # estimate value function for all states \n",
    "    logp = pi.log_prob(tensor_a, tensor_s[:T]).squeeze(2)  # compute log(pi(a_t | s_t))\n",
    "\n",
    "    with torch.no_grad():  # no need for gradients when computing the advantages and value targets\n",
    "        value_targets, advantage_estimates = compute_advantage_estimates(tensor_r, values, gamma, bootstrap=True)\n",
    "        # value_targets, advantage_estimates = compute_gae(tensor_r, values, gamma, lambda_=0.99)\n",
    "\n",
    "    L_pg = policy_gradient_loss_advantages(logp, advantage_estimates)  # compute the policy gradient loss\n",
    "    L_v = value_loss(values[:T], value_targets)  # add the value loss\n",
    "    total_loss = L_pg + L_v\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    total_loss.backward()  # backprop and gradient step\n",
    "    optim.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch %d, mean reward: %.3f, value loss: %.3f' % (epoch, tensor_r.mean(), L_v.item()))\n",
    "    mean_rewards[epoch] = tensor_r.mean()\n",
    "    v_losses[epoch] = L_v.item()\n",
    "    p_losses[epoch] = L_pg.item()\n",
    "\n",
    "train_env.close()\n",
    "    \n",
    "plot_training(mean_rewards, p_losses, v_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d58a33191bcbb9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.816343466Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_policy(pi, 128, {'N': 16, 'vis': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903328b94f546769",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.816489286Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_action_vs_angle(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119322447c03a961",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Task 4) Proximal policy optimization\n",
    "\n",
    "The training using policy gradient estimate is inherently unstable and leads to catastrophic forgetting. The idea of the proximal policy optimization (PPO) algorithm is simple: Perform the policy gradient update, but do not change the policy behaviour too much. While gradient clipping would help remedy this problem, it operates in the parameter space, which does not guarantee proximity of the behaviour (even a small change in parameters $\\theta$ can cause a large change in the action probabilities $\\pi_\\theta(a_t | s_t)$). The PPO algorithm uses clipping in the probability space instead. \n",
    "\n",
    "First, let us define \n",
    "$$\n",
    "r_\\theta(a_t | s_t) = \\dfrac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{old}}(a_t | s_t)}\n",
    "$$\n",
    "which is simply the ratio of the 'new' probabilities $\\pi_\\theta(a_t | s_t)$ divided by the 'old' probabilities $\\pi_{\\theta_{old}}(a_t | s_t)$. The 'old' probabilities correspond to $\\theta_{old}$, which are the policy parameters before the gradient update. The reason for these additional terms is that we will keep track of the 'old' parameters $\\theta_{old}$ in order to enforce the proximity of the policy behaviour.\n",
    "Note that \n",
    "$$\n",
    "\\nabla_\\theta r_\\theta(a_t | s_t) |_{\\theta=\\theta_{old}} = \\nabla_\\theta \\log(\\pi_\\theta(a_t | s_t)) |_{\\theta=\\theta_{old}}\n",
    "$$\n",
    "i.e. taking the gradient of this ratio is equivalent to taking the gradient of the policy log-probabilities when done at the initial parameters $\\theta_{old}$. This gives rise to the following equivalent objective for the advantage-based policy gradient (here omitting the mean over all states and actions)\n",
    "$$\n",
    "J_{A}(a_t, s_t) =  \\hat{A} \\cdot r_\\theta(a_t | s_t)\n",
    "$$ \n",
    "where $\\hat{A}$ is short for $\\hat{A}(s_t, a_t)$ - the advantage estimate for state $s_t$ and action $a_t$. If we were to take the gradient of this objective, we would get exactly the same expression we used in the previous task. The PPO replaces this by the following surrogate objective\n",
    "$$\n",
    "L_{PPO}(a_t, s_t) = \\min \\left( \\hat{A} \\cdot r_\\theta(a_t | s_t), \\quad \\hat{A} \\cdot \\text{clip}(r_\\theta(a_t | s_t), 1-\\epsilon, 1+\\epsilon) \\right)\n",
    "$$ \n",
    "where $\\epsilon \\in (0, 1)$ is a hyperparameter known as the clipping ratio (typically $\\epsilon = 0.2$). This objective essentially implements gradient clipping, but in the probability space. To be clear, this objective needs to be again averaged over all states and actions in the collected trajectories. The resulting policy gradient is\n",
    "$$\n",
    "\\textcolor{red}{\n",
    "\\hat{g}_{PPO} = \\frac{1}{N}\\frac{1}{T} \\sum_{i=1}^N \\sum_{t=0}^{T-1} \\nabla_\\theta \\min \\left( \\hat{A} \\cdot r_\\theta(a_t | s_t), \\quad \\hat{A} \\cdot \\text{clip}(r_\\theta(a_t | s_t), 1-\\epsilon, 1+\\epsilon) \\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "### The idea behind PPO\n",
    "To understand, let us see what happens, when the advantage is positive $\\hat{A} > 0$, i.e. we would like to increase the probability of the given action $a_t$. Then the objective reduces to \n",
    "$$\n",
    "\\hat{A} \\cdot \\min \\left(  r_\\theta(a_t | s_t), \\quad \\text{clip}(r_\\theta(a_t | s_t), 1-\\epsilon, 1+\\epsilon) \\right)\n",
    "$$\n",
    "Now, if $r_\\theta(a_t | s_t) \\leq 1+\\epsilon$, then the $\\min()$ chooses the ratio itself and the result is the same as if we used the original objective. But if $r_\\theta(a_t | s_t) > 1+\\epsilon$, then the objective is stuck at the constant value of $\\hat{A} \\cdot (1+\\epsilon)$ and therefore we get zero gradient. This means that the objective allows the policy to increase the probability of the action $a_t$ by at most $(1+\\epsilon)$-times the 'old' probability. \n",
    "\n",
    "Similarly, for a negative advantage $\\hat{A} < 0$, we would like to reduce the probability of $a_t$. The PPO objective would be \n",
    "$$\n",
    "\\hat{A} \\cdot \\max \\left(  r_\\theta(a_t | s_t), \\quad \\text{clip}(r_\\theta(a_t | s_t), 1-\\epsilon, 1+\\epsilon) \\right)\n",
    "$$\n",
    "If $r_\\theta(a_t | s_t) \\geq 1-\\epsilon$, then the $\\max$ chooses the ratio itself and nothing interesting happens. If $r_\\theta(a_t | s_t) < 1-\\epsilon$, however, then the objective is again stuck at a constant value of $\\hat{A} \\cdot (1-\\epsilon)$ and the gradient is zero again. This does not let the probability of $a_t$ fall below $(1-\\epsilon)$-times the 'old' probability. \n",
    "\n",
    "So, essentially, the PPO objective will not allow the probabilities of the actions to change too much with respect to the 'old' policy that was used to sample the trajectories.\n",
    "\n",
    "The following image illustrates the clipping objective for positive and negative advantages respectively.\n",
    "<img src=https://miro.medium.com/v2/resize:fit:1400/1*RUEQ7RXzywlV63nZ0ldJUg.png width=\"600\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e6647d180bc0f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.816638019Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: implement ppo_loss in solution.py\n",
    "from solution import ppo_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbeded103d2b55",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Quick test for your implementation is below. The output should be: tensor(-0.8250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a21cf8f6db43d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.816803018Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_p_ratios = torch.tensor([0.8, 1, 1, 0.6, 1.2, 1.4, 1.6, 0.2]).reshape((8, 1)).float()  # testing logp\n",
    "test_advantage_estimates = torch.tensor([1, 2, 1, 0, 1, 1, 0, 2]).reshape((8, 1)).float()  # testing rewards \n",
    "test_epsilon = 0.2\n",
    "\n",
    "print(ppo_loss(test_p_ratios, test_advantage_estimates, test_epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287cac7cfed7b594",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.816952696Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "N = 32\n",
    "T = 128\n",
    "config = {'N': N, 'vis': 1}\n",
    "epochs = 500\n",
    "lr = 0.01\n",
    "gamma=0.95\n",
    "epsilon = 0.2\n",
    "\n",
    "sgd_iters = 5\n",
    "\n",
    "# policy, environment and optimizer\n",
    "pi = StochasticPolicyValue(state_dim=4)\n",
    "train_env = PendulumEnv(config)\n",
    "optim = torch.optim.SGD(pi.parameters(), lr=lr)\n",
    "\n",
    "mean_rewards, p_losses, v_losses = np.zeros(epochs), np.zeros(epochs), np.zeros(epochs)  # for logging mean rewards over epochs\n",
    "for epoch in range(epochs):\n",
    "    tensor_s, tensor_a, tensor_r = sample_trajectories(train_env, pi, T)  # collect trajectories using current policy\n",
    "        \n",
    "    with torch.no_grad():  # compute the old probabilities\n",
    "        logp_old = pi.log_prob(tensor_a, tensor_s[:T]).squeeze(2)  # compute log(pi(a_t | s_t))\n",
    "    \n",
    "    for i in range(sgd_iters):  # we can even do multiple gradient steps\n",
    "        values = pi.value_estimates(tensor_s)  # estimate value function for all states \n",
    "        logp = pi.log_prob(tensor_a, tensor_s[:T]).squeeze(2)  # compute log(pi(a_t | s_t))\n",
    "\n",
    "        with torch.no_grad():  # no need for gradients when computing the advantages and value targets\n",
    "            # value_targets, advantage_estimates = compute_advantage_estimates(tensor_r, values, gamma, bootstrap=True)\n",
    "            value_targets, advantage_estimates = compute_gae(tensor_r, values, gamma, lambda_=0.97)\n",
    "            advantage_estimates = (advantage_estimates - advantage_estimates.mean()) / advantage_estimates.std()  # normalize advantages\n",
    "            \n",
    "        L_v = value_loss(values[:T], value_targets)  # add the value loss\n",
    "        \n",
    "        p_ratios = torch.exp(logp - logp_old)  # compute the ratios r_\\theta(a_t | s_t)\n",
    "        L_ppo = ppo_loss(p_ratios, advantage_estimates, epsilon=epsilon)  # compute the policy gradient loss\n",
    "        total_loss = L_v + L_ppo\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        total_loss.backward()  # backprop and gradient step\n",
    "        optim.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch %d, mean reward: %.3f, value loss: %.3f' % (epoch, tensor_r.mean(), L_v.item()))\n",
    "    mean_rewards[epoch] = tensor_r.mean()\n",
    "    v_losses[epoch] = L_v.item()\n",
    "    p_losses[epoch] = L_ppo.item()\n",
    "    \n",
    "train_env.close()\n",
    "\n",
    "plot_training(mean_rewards, p_losses, v_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea923532bc70a49",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.817168609Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_policy(pi, 512, {'N': 16, 'vis': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333da400207a06e1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T17:18:28.817338693Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_action_vs_angle(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ff0bde9c45a729",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The end!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b50f848ea09bd55",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Suplementary material\n",
    "\n",
    "Reinforcement learning explanation https://spinningup.openai.com/en/latest/index.html \n",
    "Physical simulator MuJoCo https://mujoco.readthedocs.io/en/latest/computation/index.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6ba04c1fb5921ec8bcaf85a71d847b8596bba86a12f6c6aca4ac137855d5ada"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
